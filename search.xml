<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[Thin-Slicing for Pose - Learning to Understand Pose without Explicit Pose Estimation - 论文笔记]]></title>
      <url>http://wentaoma.com/2017/04/26/paper-notes-thin-slicing/</url>
      <content type="html"><![CDATA[<p>这篇文章主要使用了DCNN构造了一个嵌入函数(embedding function)，能够感受姿态并将相似姿态的人投影到嵌入空间邻近区域中。作者说其结构能有效剔除服装、背景、和成像因素以学习鲁棒的人体姿态特征。对于各种姿态相关任务，这种方法计算高效并且不需要明确的估计姿势，绕过了定位关节的问题。作者还给出采用此方法的图像检索和姿态识别的例子。<br><a id="more"></a></p>
<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><blockquote>
<p>There can be as much value in the blink of an eye as in months of rational analysis.<br><em>眨眼间的判断和数月的理性分析具有同样的价值。</em><br>— Malcolm Gladwell</p>
</blockquote>
<p>这个句子出自Malcolm Gladwell的《Blink》，书中举了一些例子来说明人们存在一种不自觉的瞬间判断(snap judgment)与快速认知(rapid cognition)的能力称之为「薄片分析」 (thin-slicing)。</p>
<p>这篇文章就是让机器使用thin-slicing来分析人体姿态并减少对精确人体姿态估计的需求。人体姿态估计的问题主要在于定位独立的身体关节。即使全身可见，经常性的自我遮挡和姿势差异是这个问题很难解决。同时还由于复杂的结构推导问题，人体姿态估计在实践中计算繁重。</p>
<p>作者认为很多问题都只需要估计姿势相似度因此准确的人体姿势估计可能并不是必须的。因此改为基于CNN训练一个有效的姿态嵌入函数。</p>
<p>关于嵌入(embedding)是一个数学上的概念：<br>Embedding在数学上表示一个maping, f: X -&gt; Y， 也就是一个function，其中f是单射函数，而且有structure-preserving (结构保存，比如在X所属的空间上X1 &lt; X2,那么映射后在Y所属空间上同理 Y1 &lt; Y2，具体保存什么结构要看问题来定)的特性。</p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p><strong>Pose estimation</strong><br><strong>Action recognition with pose</strong><br><strong>Embedding by similarity</strong><br><strong>Pose embedding</strong></p>
<h1 id="算法概览"><a href="#算法概览" class="headerlink" title="算法概览"></a>算法概览</h1><p>论文在训练CNN时，使用了Triplet Rank Loss：</p>
<p>论文对full-body和upper-body的姿态分别训练了两个嵌入函数，两个使用同样网络结构。</p>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><img src="/2017/04/26/paper-notes-thin-slicing/net.jpg" alt="net.jpg" title="">
<p>图片中蓝色卷积层使用VGG-S的预训练权值并在训练时微调, 来源于这篇论文：Return of the Devil in the Details: Delving Deep into Convolutional Nets</p>
<h2 id="Triplet-rank-loss"><a href="#Triplet-rank-loss" class="headerlink" title="Triplet rank loss"></a>Triplet rank loss</h2><p>Triplet loss即使用3个样本计算loss： Anchor,Positive(和Anchor属于同一类)和Negative(和Anchor属于不同类)；由3个样本经过网络得到3个特征表达f(a),f(p),f(n)。其优化目标是让f(a)和f(p)距离L(a,p)尽可能小，f(a)和f(n)距离L(a,n)尽可能大，且这两个距离之间至少相差α。最后Loss取L(a,p)-L(a,n)+α的正值，负值0.<br>关于Triplet Loss的详细内容参见<a href="http://blog.csdn.net/tangwei2014/article/details/46788025" target="_blank" rel="external">Triplet loss 原理以及梯度推导</a></p>
<p>论文中Loss方程如下:<br><img src="/2017/04/26/paper-notes-thin-slicing/loss.jpg" alt="loss.jpg" title=""></p>
<h1 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h1><h2 id="数据源"><a href="#数据源" class="headerlink" title="数据源"></a>数据源</h2><p>作者收集了有关节标记的姿态图片，并根据标记关节坐标计算姿态间距，主要基于 MPII Human Pose dataset，H3D 和 VOC2009 person (trainval) 做补充。</p>
<p>最后数据集大小为：<br>12,366 images for training (MPII: 10,000, H3D: 843, VOC2009 people: 1,523)<br>9,919 images for validation (MPII: 9,919)</p>
<h2 id="图像和姿态标准化"><a href="#图像和姿态标准化" class="headerlink" title="图像和姿态标准化"></a>图像和姿态标准化</h2><p>全身：使用共同的姿势中心点对齐图像(如骨盆)，裁剪出最小包含整个人体框的的正方形<br>上身：以头部为宽度中心点，按比例裁剪正方形</p>
<p>两个图像姿态距离定义为：两个图像对应关节坐标欧氏距离均值</p>
<h1 id="Learning-pose-embedding-network"><a href="#Learning-pose-embedding-network" class="headerlink" title="Learning pose embedding network"></a>Learning pose embedding network</h1><h2 id="Triplet-sampling"><a href="#Triplet-sampling" class="headerlink" title="Triplet sampling"></a>Triplet sampling</h2><p>数万张图像之间的3元组配对数量太多，因此：<br>对每个图像x，选择p个最邻近的图像作为Positive样本的集合P，其余图像作为Negative样本集合N；在每个epoch后移除Negative集合中最远的一些样本。</p>
<h2 id="Learning-network-with-random-triplets"><a href="#Learning-network-with-random-triplets" class="headerlink" title="Learning network with random triplets"></a>Learning network with random triplets</h2><p>为了更有效的求解triplet rank loss,在一个mini-batch中先算出anchor和每个negative、positive之间的距离，这样每个图像只需前向传播一次。同时由于每个图像可能参与到多个triplet中，因此将每个的梯度累加最后执行一次反向传播。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>作者在MPII上测试了图像检索表现，VOC2012 Action dataset及PPMI上测试了动作识别的表现。</p>
<h2 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h2><p>对于全身姿态嵌入，sampling中p取30，上身的p取15.<br>mini-batch大小设置为128：1个anchor，5个随机positive，122个随机negative。Negative样本集合N大小每epoch减小3K直至减小到1K。每个batch中图像单独变化和拉伸±10%，对于整个batch以0.5的概率随机翻转。<br>FC layer学习率初始为0.01，每epoch乘0.2，卷积层学习率是FC的1/10。 momentum和weight decay设为0.9和0.0005.</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><ol>
<li><p>在图像搜索上性能比对比的几个模型都要好，但是在没有学过图像中类似姿势情况下的图像检索性能明显差于明确姿态估计的方法。</p>
<img src="/2017/04/26/paper-notes-thin-slicing/mpii.jpg" alt="mpii.jpg" title="">
</li>
<li><p>在动作识别时，使用SVM做分类器。只是用embedding提取的特征时准确率很低，主要是学习时没有学习人与物体之间交互而只学习了人体姿势，但如果结合VGG-16和VGG-19提取的特征（即取FC2的输出）准确率比VGG原本的准确率要高。</p>
<img src="/2017/04/26/paper-notes-thin-slicing/voc2012.jpg" alt="voc2012.jpg" title="">
</li>
</ol>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这个在姿态相关的图像检索上能直接应用，对于动作识别来说更多的是作为辅助的特征提取方式。</p>
]]></content>
      
        
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> PaperNotes </tag>
            
            <tag> CVPR16 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Deep Residual Learning for Image Recognition - 论文笔记]]></title>
      <url>http://wentaoma.com/2017/04/23/paper-notes-resnet/</url>
      <content type="html"><![CDATA[<p>原文地址:<a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="external">Deep Residual Learning for Image Recognition</a><br>ResNet采用了深度残差学习的方法将网络深度大大加深，在实现过程中作者使用了诸如恒等映射等先验知识很值得参考。<br><a id="more"></a></p>
<h1 id="论文背景"><a href="#论文背景" class="headerlink" title="论文背景"></a>论文背景</h1><p>深度卷积神经网络在图像分类领域带来了一系列突破。深度网络可以很自然的将低/中/高层特征和分类器整合进一个端到端(end to end)的多层模型中，而特征的“级别”(“levels”)可以通过堆叠层的数量（深度）来丰富。最近结果显示，模型的深度发挥着至关重要的作用，这样导致了ImageNet竞赛的参赛模型都趋向于“非常深”——16 层 到30层 。许多其它的视觉识别任务的也都得益于非常深的模型。</p>
<p>在深度的重要性的驱使下，出现了一个新的问题：训练一个更好的网络是否就像堆叠更多的层一样简单呢？解决这一问题的障碍之一便是困扰人们很久的梯度消失/梯度爆炸，这从一开始便阻碍了模型的收敛。归一初始化（normalized initialization）和中间归一化（intermediate normalization）在很大程度上解决了这一问题，它使得采用SGD反向传播算法的数十层的网络能够开始收敛。</p>
<p>当深层网络可以开始收敛时，又出现了另一个问题——退化(degradation)：随着网络深度的增加，准确率增加到一定程度后反而开始迅速衰减。然而，这种现象并不是由过拟合造成的：因为在一个适当的模型中增加更多的层却导致了如下图所示的更高的错误率:<br><img src="/2017/04/23/paper-notes-resnet/figure1.jpg" alt="figure1.jpg" title=""></p>
<p>退化的出现表明了并非所有的系统都是容易优化的。因此作者提出了深度残差学习来解决退化问题：</p>
<h1 id="到底什么是深度残差学习"><a href="#到底什么是深度残差学习" class="headerlink" title="到底什么是深度残差学习"></a>到底什么是深度残差学习</h1><p>首先考虑这样一种情况：<br>对于一个深层版本网络，如果只使用恒等映射(identity mapping来构建新增加的层，其它的层由浅层版本中直接复制而来——即让新增加的层表达式为f(x)=x 。这时候将浅层网络视为映射H(x)，当深层网络新增加的层都为恒等映射f(x’)=x’时：新增加的层接受底层输入为x’=H(x)，因此深层网络最后输出应为 f(H(x))=H(x) 和浅层网络一样。<br>在这种情况下，一个更深的模型不应当产生比它的浅层版本更高的训练错误率。</p>
<p>那么之前的退化现象又是怎么回事？这可能是因为增加的多个非线性层很难来估计出一个恒等映射。</p>
<p>为了解决这一点，作者基于<strong>增加层如果为恒等映射那么更深层网络不应该比浅层网络产生更高错误率</strong>的思想，提出了深度残差学习：</p>
<p>即使用如下图所示的模块来增加网络深度：<br><img src="/2017/04/23/paper-notes-resnet/building-block.jpg" alt="building-block.jpg" title=""><br>这个模块直接通过shorcut connections方式为增加层引入了一个恒等映射，让原来的卷积层去学习拟合另一个映射F(x)</p>
<p>如果我们将浅层网络视作映射H(x)，深层网络视作H’(x)的话，那么由于增加的层直接引入了恒等映射，所以H’(x)=H(x)+F(x)</p>
<p>极端情况下，即恒等映射是增加层的最优解，那么直接将F(x)置为0（卷积层权重、偏差为0）即可，远比使用多个非线性层来拟合恒等映射简单。</p>
<p>作者将这样一个F(x)称之为残差映射(residual mapping)。参考极端情况下的例子，作者推断残差映射比原始未参考的映射(unreferenced mapping)更容易优化。</p>
<p>同时出于计算量的考量，作者在之前结构上修改提出了bottleneck的结构：<br><img src="/2017/04/23/paper-notes-resnet/bottleneck.png" alt="bottleneck.png" title=""></p>
<p>这种构造网络的方式即称为深度残差学习。</p>
<h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p>整体参考VGGNet，<br>卷积层主要为3*3的滤波器，并且遵循：输出特征尺寸相同的层含有相同数量的滤波器；如果特征尺寸减半，则滤波器的数量增加一倍来保证每层的时间复杂度相同。<br>直接通过stride为2的卷积层来进行下采样。<br>分类使用使用全局平均池化和一个1000类的全连接层代替VGG的三个全连接层。</p>
<img src="/2017/04/23/paper-notes-resnet/arch.png" alt="arch.png" title="">
<p>在上图每个用大括号的卷积层都使用shorcut连接。</p>
<p>模型构建好后进行实验，在plain上观测到明显的退化现象，而且ResNet上不仅没有退化，34层网络的效果反而比18层的更好，而且不仅如此，同等层数下ResNet的收敛速度比plain的要快得多。<br><img src="/2017/04/23/paper-notes-resnet/test.png" alt="test.png" title=""></p>
<p>对于shortcut的方式，作者提出了三个选项：<br>A. 使用恒等映射，如果residual block的输入输出维度不一致，对增加的维度用0来填充；<br>B. 在block输入输出维度一致时使用恒等映射，不一致时使用线性投影以保证维度一致；<br>C. 对于所有的block均使用线性投影。<br>对这三个选项都进行了实验，发现虽然C的效果好于B的效果好于A的效果，但是差距很小，因此线性投影并不是必需的，而使用0填充时，可以保证模型的复杂度最低，这对于更深的网络是更加有利的。</p>
<h1 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h1><p>数据扩增：</p>
<ol>
<li>调整图像的大小使它的短边长度随机的从[256,480]中采样来增加图像尺度。</li>
<li>从一张图像或者它的水平翻转图像中随机采样一个224*224的crop，每个像素都减去均值。图像使用标准的颜色增强</li>
</ol>
<p>在每一个卷积层之后，激活层之前均使用batch normalization。<br>根据 <a href="https://arxiv.org/abs/1502.01852" target="_blank" rel="external">https://arxiv.org/abs/1502.01852</a> 来初始化权值然后从零开始训练所有plain/残差网络。<br>我们使用的mini-batch的尺寸为256。学习率从0.1开始，每当错误率平稳时将学习率除以10，整个模型进行60∗10^4次迭代训练。我们将权值衰减设置为0.0001，动量为0.9。不使用dropout</p>
<p>在测试中，采取标准的10-crop（四角+中心*翻转）测试。<br>同时使用全卷积形式，并在多个尺度（{224,256,384,480,640}）的结果上取平均分。</p>
]]></content>
      
        
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> PaperNotes </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Going deeper with convolutions - 论文笔记]]></title>
      <url>http://wentaoma.com/2017/04/20/paper-notes-inceptionv1/</url>
      <content type="html"><![CDATA[<p>原文地址:<a href="https://arxiv.org/abs/1409.4842v1" target="_blank" rel="external">Going deeper with convolutions</a><br>GoogLeNet(Inception v1)通过重新设计网络结构在维持计算消耗不变同时增加了网络宽和深。<br><a id="more"></a></p>
<h1 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h1><p>最近(2014)三年，主要由于深度学习和越来越实际的卷积网络的发展，图像识别以及物体检测的质量都在飞速提高。大多数进步并不只是更强大的硬件、更大的数据库和模型所带来的，而主要是一些新创意、新算法，以及优化的网络结构的成果。随着移动计算和嵌入式计算得到越来越广泛的认同，我们的算法的效率——尤其是其能量和存储利用率——变得越来越重要。值得注意的是，这篇文章中展现的深度结构在设计时就考虑了这些因素(大部分时候前馈模型计算量限制15亿次乘加运算左右)，而不仅是执着于单纯提高精度。</p>
<h1 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h1><p>从LeNet-5开始，CNN通常就使用一种标准结构：一些堆叠的卷积层（后面通常跟随归一化和最大池化层）的后面接上一个或多个全连接层。采用这种基本设计的网络越来越流行并在MNIST、CIFAR尤其是ImageNet分类竞赛取得了最佳结果。 对于如ImageNet等较大的数据集，最近(2014)的趋势是增加层的数量和大小同时使用dropout方法解决过拟合。</p>
<p>GoogLeNet作为一个22层的网络模型，借鉴了很多Network in Network中的思想：<br>使用带有ReLU的1x1的卷积，主要为了降维以降低运算量，增大网络规模，加深加宽网络。</p>
<h1 id="3-设计动机与考虑"><a href="#3-设计动机与考虑" class="headerlink" title="3 设计动机与考虑"></a>3 设计动机与考虑</h1><p>最直接提高深度神经网络性能的方法是增加其规模，包括通过增加层数以增大深度，通过增加每一层的节点数以增加宽度。</p>
<p>然而这种简单的解决方法有两大缺陷：</p>
<ol>
<li>更大的网络规模往往意味着更多的参数，这使得扩大后的网络更易过拟合</li>
<li>增大网络还会带来计算资源需求的暴增，如果增加计算的部分没有被有效使用（比如大部分的权值趋于0），那么大量的宝贵计算能力会被浪费</li>
</ol>
<p>解决这两个问题的基本方法最终一般是把全连接改成稀疏连接的结构，甚至在卷积层也采用稀疏连接的结构。（采用了赫布原则(Hebbian principle)的思想）<br>然而当涉及大量非统一的（non-uniform）稀疏的数据结构的计算时，现在的计算设备效率很低。因此目前大部分面向机器学习的系统都利用卷积的优势在空间域中使用稀疏性。然而如果使用AlexNet的结构，那么为了更好的优化并行计算又有返回全连接方式的趋势。为了均衡计算效率和模型稀疏性，作者提出了Inception结构。</p>
<h1 id="4-结构细节"><a href="#4-结构细节" class="headerlink" title="4 结构细节"></a>4 结构细节</h1><p>Inception模块是一层一层往上栈式堆叠的，所以它们输出的关联性统计会产生变化：更高层抽象的特征会由更高层次所捕获，而它们的空间聚集度会随之降低，因为随着层次的升高，3×3和5×5的卷积的比例也会随之升高。如下图所示：<br><img src="/2017/04/20/paper-notes-inceptionv1/naive-inception.jpg" alt="naive-inception.jpg" title=""></p>
<p>然而这样的结构会导致计算量的爆炸增长，因此作者采用1x1的卷积在3x3和5x5卷积之前降维:<br><img src="/2017/04/20/paper-notes-inceptionv1/inception.jpg" alt="inception.jpg" title=""></p>
<p>该设计还符合了：视觉信息应该被多层次处理，然后被汇集到下面层次汇总，同时抽取多尺度特征的思想。</p>
<h1 id="5-GoogLeNet"><a href="#5-GoogLeNet" class="headerlink" title="5 GoogLeNet"></a>5 GoogLeNet</h1><p>网络各层信息如下所示：<br><img src="/2017/04/20/paper-notes-inceptionv1/googlenet.jpg" alt="googlenet.jpg" title=""><br>其中3x3 reduce, 5x5 reduce表示在相应卷积之前进行降维的1x1卷积。<br>对于每一个Inception模块，其内部的卷积及池化(pool proj)都做了维持输入输出图像长宽不变的padding，最后将1x1, 3x3, 5x5, pool proj的输出（因为其长宽均相同）的通道叠加作为整个Inception模块输出。</p>
<p>如果只计算包含参数的层，整个网络有22层；如果算上池化层则有27层。</p>
<p>同时参考Network in Network，GoogLeNet采用了全局平均池化代替全连接层（但仍保留了一个线性层方便正对其它标签数据集fine-tune）进行分类因此Top1的准确率提高了0.6%，但dropout还是需要的。</p>
<p>因为网络较深而且GoogLeNet中层产生的特征也具有很好区分度，所以GoogLeNet在中间层(INception4a, Inception4d)加了两个辅助分类器，训练时辅助分类器的loss*0.3加回到总loss中，在预测时不适用辅助分类器。</p>
<h1 id="6-训练方法"><a href="#6-训练方法" class="headerlink" title="6 训练方法"></a>6 训练方法</h1><p>GoogLeNet采用了<a href="http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf" target="_blank" rel="external">Large scale distributed deep networks</a>提出的分布置信网络(DistBelief)。</p>
<p>使用Momentum=0.9的SGD，learning rate每8个epoch降低4%。</p>
]]></content>
      
        
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> PaperNotes </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Very deep convolutional networks for large-scale image recognition - 论文笔记]]></title>
      <url>http://wentaoma.com/2017/04/18/paper-notes-vggnet/</url>
      <content type="html"><![CDATA[<p>原文地址:<a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="external">Very deep convolutional networks for large-scale image recognition</a><br>VggNet在ILSVRC-2014上分别获得定位和分类项目中第一名和第二名的成绩，其主要采用3x3的小卷积并将模型深度增加到16-19层。<br><a id="more"></a></p>
<h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h1><p>卷积网络(ConvNets)最近(2014)在大规模的图像和视频识别中获得了很大的成功，这可能由于大型公共图像库（如 ImageNet）以及高性能计算系统（如GPU或大规模分布式集群）。特别是在深度视觉识别结构发展中扮演了重要角色的ImageNet大规模视觉识别大赛(ImageNet Large-ScaleVisual Recognition Challenge, ILSVRC)为从高维浅层特征编码 (ILSVRC-2011冠军)到深度卷积网络(ILSVRC-2012冠军)的几代大规模图像分类系统提供了测试平台。</p>
<p>随着卷积网络在计算机视觉领域的应用越来越广泛，为了获得更高的准确率，越来越多的人尝试在<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" target="_blank" rel="external">Krizhevsky et al. (2012)</a>中的原始框架上进行改进。例如，ILSVRC-2013最好的参赛模型在第一个卷积层上使用了较小的接受域窗口(smaller receptive window size)以及较小的滑动步长。另一种改进方案是在整张图像及其多个尺度上稠密的训练和测试网络。本文中，我们关注了卷积网络结构设计中的另一个重要因素——深度。为此，我们固定了网络框架的其他参数，然后逐步通过增加更多的卷积层来增加网络的深度。由于我们在所有层都是用非常小(3×3)的卷积滤波器，因此这是可行的方法。</p>
<h1 id="2-卷积网络设置"><a href="#2-卷积网络设置" class="headerlink" title="2 卷积网络设置"></a>2 卷积网络设置</h1><p>作者为了在公平的环境下衡量由增加的卷积层深度所带来的效果，所有的卷积层都用相同的原则来设计。</p>
<h2 id="2-1-结构"><a href="#2-1-结构" class="headerlink" title="2.1 结构"></a>2.1 结构</h2><p>在训练阶段，采用224x224的固定大小RGB图像做输入，唯一的预处理操作就是计算出整个训练集的每个像素RGB均值。（此处预处理和ALexNet相同）。</p>
<p>输入图像被送进使用3x3小卷积的卷积层栈中。在某一个网络设置中，作者也使用了1x1的卷积核，1x1的卷积核可以看作是对输入通道做了一个线性变换(之后接一个非线性操作)。所有卷积都使用1像素步长，同时对输入做相应的padding操作以保持经过卷积后的输出图像空间分辨率(图像长/宽)不变，比如对于3x3的卷积层就要在图像四周做1像素的padding。空间池化包含5个最大池化层，这些池化层在某一些卷积层后(而不是所有)。最大池化采用步长为2的2x2大小窗口。</p>
<p>卷积层栈之后有3个全连接(FC)层：前两层各有4096个通道，第三层用于做1000类的ILSVRC分类因而有1000个通道。最后一层是softmax层。全连接层的设置在作者的所有网络中都相同。</p>
<p>所有的隐藏层都进行ReLU操作。同时我们所有网络中只有一个使用了LRN（LRN参数和ALexNet中相同），因为作者发现LRN并不会提高在ILSVRC上的准确度但会增加内存消耗和计算时间。</p>
<h2 id="2-2-配置"><a href="#2-2-配置" class="headerlink" title="2.2 配置"></a>2.2 配置</h2><p>这篇文章所评估的卷积网络配置如下：<br><img src="/2017/04/18/paper-notes-vggnet/nets.png" alt="nets.png" title=""><br>所有的网络都采用2.1中描述的设计，它们之间只有深度上的差异。卷积层的通道数从64开始每次经过最大池化层后x2直到512.</p>
<p>各网络的参数数量如下：<br><img src="/2017/04/18/paper-notes-vggnet/params.png" alt="params.png" title=""><br>尽管网络很深，但是权重的数量并不比一个网络更浅但是卷积核尺寸更大的模型多。</p>
<h2 id="2-3-讨论"><a href="#2-3-讨论" class="headerlink" title="2.3 讨论"></a>2.3 讨论</h2><p>VGG使用了更小的(3x3，步长为1)的卷积核，不同于前两年的ILSVRC的优胜者(2012-AlexNet: 11x11步长4, <a href="https://arxiv.org/abs/1311.2901" target="_blank" rel="external">2013; 2013-ZFNet</a>: 7x7步长2)。显然，两个连续3x3卷积相当于一个5x5的卷积，而三个3x3的卷积则相当于7x7的卷积.</p>
<p>使用3x3小卷积的好处有两个，以用3个3x3卷积替代一个7x7卷积为例：</p>
<ol>
<li>3个卷积层就可以做3次ReLU等非线性操作而不是1次，这使模型Capacity增加了。</li>
<li>多个小卷积叠加带来的是更少的参数量，如果每个卷积层输入输出都有C个通道，则3个3x3卷积层权重数为3(3^2<em>C^2)=27</em>C^2,而单一的7x7卷积层权重数为7<em>7</em>C^2=49*C^2</li>
</ol>
<p>另外作者还考虑到了1×1的卷积，尽管(输出和输出的通道数量相同的)1x1卷积层本质上相当于到相同维度空间的一个线性投影，但是激活函数使整个模型非线性增加。1×1卷积层最近(2014)被使用在的<a href="https://arxiv.org/abs/1312.4400" target="_blank" rel="external">Network in Network</a>结构中。</p>
<p>小尺寸的卷积滤之前在<a href="https://www.researchgate.net/publication/220812758_Flexible_High_Performance_Convolutional_Neural_Networks_for_Image_Classification" target="_blank" rel="external">Flexible, High Performance Convolutional Neural Networks for Image Classification(2011)</a>中使用过，但是他们的网络远没有VGGNet的深。</p>
<h1 id="3-分类框架"><a href="#3-分类框架" class="headerlink" title="3 分类框架"></a>3 分类框架</h1><h2 id="3-1-训练"><a href="#3-1-训练" class="headerlink" title="3.1 训练"></a>3.1 训练</h2><p>VGGNet使用的训练方法基本延续AlexNet的方法（除了从多尺度图像中裁切，后文会有解释）：<br>使用带有动量(momentum=0.9)的mini-batch(batch_size=256)梯度下降法(基于反向传播, <a href="http://yann.lecun.org/exdb/publis/pdf/lecun-89e.pdf" target="_blank" rel="external">LeCun et al, 1989</a>)来优化多项式回归(the multinomial logistic regression)。通过权值衰减(L2惩罚系数设置为5*10^−4)以及对前两个全连接层执行dropout(dropout=0.5)来对训练进行正则化。初始学习率设置为10^−2，当验证集准确率稳定时将学习率除以10。学习率总共降低了3次，训练一共进行了370K次迭代(74个epoch)。相比AlexNet约90epochs有所减少，由于：1) 更深的深度和更小的卷积滤波器尺寸隐式的增强了正则化；2) 某些层执行了预初始化(pre-initialisation)。</p>
<p>网络权重的初始化是非常重要的，由于深度网络梯度的不稳定性，不合适的初始化将会阻碍网络的学习。为了避免这个问题，我们先在网络A上使用随机初始化进行训练。然后在训练更深的网络时，我们使用网络A来初始化前四个卷积层和最后三个全连接层(中间层使用随机初始化)。同时作者并没有降低预初始化层的学习率。采用随机初始化的层则从均值0方差0.01的正态分布中对权重进行采样，bias初始化为0。作者在文章提交后发现可以使用<a href="http://proceedings.mlr.press/v9/glorot10a.html" target="_blank" rel="external">Understanding the difficulty of training deep feedforward neural networks</a>中的随机初始化程序来对权重进初始化而不需要进行预训练。</p>
<h3 id="训练图像大小"><a href="#训练图像大小" class="headerlink" title="训练图像大小"></a>训练图像大小</h3><p>作者在模型训练时，使用了Multi-scale的训练：把原始图像缩放到最小边S不小于224，然后在整幅图像上提取224*224片段来进行训练。两种方案：<br>方案1：所有图像上固定S，分别设置S=256，和S=384，然后进行裁切来训练两个模型，使用两种模型来评估。<br>方案2：对于每一幅图像，在[Smin,Smax]中随机选取一个S，然后在进行裁切来训练模型，这种训练方式相当于使用了尺寸抖动（scale jittering）的数据增强，可以使用一个单一的模型来对多尺寸图像进行识别。</p>
<h2 id="3-2-测试"><a href="#3-2-测试" class="headerlink" title="3.2 测试"></a>3.2 测试</h2><p>测试阶段，图像缩放到一个尺寸Q（Q与训练尺寸S并不一定要相同）。然后根据<a href="https://arxiv.org/abs/1312.6229" target="_blank" rel="external">OverFeat</a>中方法，将网络转换为全卷积网络(FCN):第一个全连接层转换成7×7的卷积层，后两个全连接层转换成1×1的卷积层。再将此网络上计算缩放后但未经裁切的图片的分类得分，计算原始图像和翻转图像上得分平均值作为最终得分。[这部分可以参考: <a href="http://www.jianshu.com/p/6d441e208547" target="_blank" rel="external">http://www.jianshu.com/p/6d441e208547</a>]</p>
<p>作者还发现在测试时使用多种尺寸缩放图像并进行裁剪再结合上述方法可以进一步提高准确率，并给出了实验结果(4.3)。</p>
<h2 id="3-3-实现细节"><a href="#3-3-实现细节" class="headerlink" title="3.3 实现细节"></a>3.3 实现细节</h2><p>VGGNet基于Caffe实现但对其进行了修改，以满足多GPU训练和评估的需要。</p>
<p>训练时，将每批数据分给多个GPU并行计算，最后计算所有GPU的梯度平均值作为最后此批数据总梯度。因为梯度计算在GPU间同步所以多GPU训练和单GPU训练结果相同。</p>
<p>训练时间：4块NVIDIA Titan Black(是单卡的3.75倍)训练2-3周</p>
<h1 id="4-分类实验"><a href="#4-分类实验" class="headerlink" title="4 分类实验"></a>4 分类实验</h1><h2 id="4-1-单一尺寸-SINGLE-SCALE-EVALUATION"><a href="#4-1-单一尺寸-SINGLE-SCALE-EVALUATION" class="headerlink" title="4.1 单一尺寸(SINGLE SCALE EVALUATION)"></a>4.1 单一尺寸(SINGLE SCALE EVALUATION)</h2><img src="/2017/04/18/paper-notes-vggnet/single-scale.png" alt="single-scale.png" title="">
<h2 id="4-2-多尺寸-MULTI-SCALE-EVALUATION"><a href="#4-2-多尺寸-MULTI-SCALE-EVALUATION" class="headerlink" title="4.2 多尺寸(MULTI-SCALE EVALUATION)"></a>4.2 多尺寸(MULTI-SCALE EVALUATION)</h2><img src="/2017/04/18/paper-notes-vggnet/multi-scale.png" alt="multi-scale.png" title="">
<h2 id="4-3-多尺寸-裁剪-MULTI-CROP-EVALUATION"><a href="#4-3-多尺寸-裁剪-MULTI-CROP-EVALUATION" class="headerlink" title="4.3 多尺寸+裁剪(MULTI-CROP EVALUATION)"></a>4.3 多尺寸+裁剪(MULTI-CROP EVALUATION)</h2><img src="/2017/04/18/paper-notes-vggnet/multi-crop.png" alt="multi-crop.png" title="">
<h2 id="4-4-多卷积网络融合"><a href="#4-4-多卷积网络融合" class="headerlink" title="4.4 多卷积网络融合"></a>4.4 多卷积网络融合</h2><img src="/2017/04/18/paper-notes-vggnet/net-fusion.png" alt="net-fusion.png" title="">
]]></content>
      
        
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> PaperNotes </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[ImageNet Classification with Deep ConvolutionalNeural Networks - 论文笔记]]></title>
      <url>http://wentaoma.com/2017/04/17/paper-notes-alexnet/</url>
      <content type="html"><![CDATA[<p>原文地址:<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" target="_blank" rel="external">ImageNet Classification with Deep ConvolutionalNeural Networks</a><br>AlexNet是2012年ILSVRC的冠军, Top1和Top5的Error Rate分别为: 37.5%, 17.0%.<br><a id="more"></a></p>
<h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h1><p>在此论文发表时， 基于已标记的小型图片数据集(如：NORB, Caltech-101/256, and CIFAR-10/100)的简单物体识别工作已经取得了相当优异的结果。例如MNIST数字识别的当前(即论文发表时间:2012)最优异的Error Rate(&lt;0.3%)已经超越了人类的水平。<br>然而真实情况下的物体识别要更复杂，同时也需要更大的数据集。但是这种大量的已标注的数据集直到最近才有收集到的可能，包括LabelMe和ImageNet等。</p>
<p>为了从大量图片数据中学习，需要一个具有巨大Learning Capacity的模型。同时因为物体识别的复杂性，其问题即使诸如ImageNet这种量级的数据集也不可能完全指定，因此选用的模型也必须有较好的泛化能力。卷积神经网络(CNN)则是满足这些要求的一种模型。CNN的Capacity可以由它的的深度和广度来控制，同时对于自然图像其也能给出强且几乎正确的预测。另外相对于拥有近似层数的标准前馈神经网络，CNN由于更少的连接数和参数个数因而更容易训练同时只会略微降低理论最佳表现。</p>
<p>尽管CNN有如此之多的好处，但将其大规模用于高分辨率图像上仍然花费巨大。幸运的是，如今(2012)的GPU对2D卷积计算进行了大量优化，已经足够训练大型CNN，而且最近(2012)诸如ImageNet的数据集也包含了足够多的已标记数据可以训练如此规模的模型同时又不会引起严重的过拟合。</p>
<h1 id="2-数据集"><a href="#2-数据集" class="headerlink" title="2 数据集"></a>2 数据集</h1><p>ImageNet的图像分辨率并不固定，而AlexNet需要一个固定维数的输入。因此，作者将图片降采样到固定分256x256的分辨率上。<br>对于一个给定图像，首先将最短边(保持比例)拉伸到256个像素长度，再从拉伸后图像裁剪出中间的一个256x256像素的区块。然后除了在整个训练集图像上计算出的每个像素均值以外，对图像不再做其他操作。总的来说作者基于中心化的原始RGB像素值训练网络。</p>
<h1 id="3-网络结构"><a href="#3-网络结构" class="headerlink" title="3 网络结构"></a>3 网络结构</h1><p>ALexNet共有8个层：5个卷积层和3个全连接层。<br>以下将按照论文作者对于其重要性的评估先后描述一些AlexNet中新的或原先不常用的特性：</p>
<h2 id="3-1-ReLU非线性激活函数"><a href="#3-1-ReLU非线性激活函数" class="headerlink" title="3.1 ReLU非线性激活函数"></a>3.1 ReLU非线性激活函数</h2><p>原先常用的饱和非线性激活函数 f(x) = tanh(x) 或 f(x) = 1/(1 + e^-x)在使用梯度下降法训练时远远慢于使用不饱和非线性激活函数 f(x) = max(0, x) 称之为Rectified Linear Unit（ReLU）.</p>

<p>对一个在CIFAR-10上训练的四层卷积神经网络，ReLU达到25%错误率的时间比tanh快了6倍。</p>
<h2 id="3-2-多GPU训练"><a href="#3-2-多GPU训练" class="headerlink" title="3.2 多GPU训练"></a>3.2 多GPU训练</h2><p>单张GTX 580只有3GB显存，这限制了可在其上训练的网络最大大小。 因此我们将网络分在两张GPU上。</p>
<p>这里还有一个额外的技巧：GPU之间只在特定的层通信。例如第三层卷积核使用第二层的所有输出做输入，而第四层的则只使用同一GPU上第三层的输出做输入。</p>
<h2 id="3-3-局部响应归一化（Local-Response-Normalization，-LRN）"><a href="#3-3-局部响应归一化（Local-Response-Normalization，-LRN）" class="headerlink" title="3.3 局部响应归一化（Local Response Normalization， LRN）"></a>3.3 局部响应归一化（Local Response Normalization， LRN）</h2><p>先看方程式：<br><img src="/2017/04/17/paper-notes-alexnet/lrn-func.jpg" alt="lrn-func.jpg" title=""></p>
<p>这个方程式对空间位置相同的n个相邻卷积核(已经过ReLU变换的)输出做Normalization操作，N则是这一层卷积核数量。<br>常量k, n, α, β是超参数。这种操作启发于真实神经元的侧抑制机制(lateral inhibition)。</p>
<p>更为具体的操作过程如下图所示：<br><img src="/2017/04/17/paper-notes-alexnet/lrn.jpg" alt="lrn.jpg" title=""></p>
<p>不过根据<a href="http://cs231n.github.io/convolutional-networks/#normalization-layer" target="_blank" rel="external">CS231n</a>的描述：这些归一化层已经不太常用，应为在实践中它们的作用较小。</p>
<h2 id="3-4-重叠池化（Overlapping-Pooling）"><a href="#3-4-重叠池化（Overlapping-Pooling）" class="headerlink" title="3.4 重叠池化（Overlapping Pooling）"></a>3.4 重叠池化（Overlapping Pooling）</h2><p>传统的Pooling层是不重叠的，而本论文提出使Pooling层重叠可以降低错误率（此处理使Top 1/5错误率下降了0.4/0.3%），而且对防止过拟合有一定(slightly)的效果。</p>
<h2 id="3-5-整体结构"><a href="#3-5-整体结构" class="headerlink" title="3.5 整体结构"></a>3.5 整体结构</h2><img src="/2017/04/17/paper-notes-alexnet/alexnet-arch.jpg" alt="alexnet-arch.jpg" title="">
<p>其中第一层输入大小采用227x227更为合适(这样就不用padding)，根据<a href="http://cs231n.github.io/convolutional-networks/#MathJax-Element-14-Frame" target="_blank" rel="external">CS231n</a>此处作者可能采用了3个像素的zero-padding，有96个11x11x3步长为4的卷积核（每个GPU有48个），第二层接受（响应归一化和池化后的）底层的输出，第3、4、5层间则没有Normalization和Pooling层。最后每个全连接层有4096个神经元。</p>
<h1 id="4-降低过拟合"><a href="#4-降低过拟合" class="headerlink" title="4 降低过拟合"></a>4 降低过拟合</h1><p>AlexNet模型共有6000万个参数，现有数据不足以学习这么多参数而不引起过拟合，作者主要采用两种方法来对抗过拟合：</p>
<h2 id="4-1-数据扩增（Data-Augmentation）"><a href="#4-1-数据扩增（Data-Augmentation）" class="headerlink" title="4.1 数据扩增（Data Augmentation）"></a>4.1 数据扩增（Data Augmentation）</h2><p>最简单和常见的减少过拟合的方法就是对图像数据做标签不变变换(label-preserving transformations)来人为增大数据集。<br>在作者实现中，在CPU上使用Python代码生成变换后的图像（而不存储在硬盘上）同时GPU根据前一个batch的数据在做训练。</p>
<p>第一种方法是图像裁剪和水平翻转。从256x256中裁出随机位置的224x224像素并加上水平翻转。在测试时使用4角+中心+翻转的10张图预测值取平均。</p>
<p>第二种方法是加调整图像RGB通道强度。对整个训练集求来RGB值的PCA，然后不降维但对特征向量乘上一个均值0标准差0.1的高斯随机变量。此处理使top 1错误率下降了1%。</p>
<h2 id="4-2-Dropout"><a href="#4-2-Dropout" class="headerlink" title="4.2 Dropout"></a>4.2 Dropout</h2><p>整合多个模型的预测结果对于减少错误率有显著效果，但这样对于花费数天来训练的神经网络来说花费巨大。但是一种更加高效的方法就是使用dropout。</p>
<p>把神经元的输出以0.5的概率置零，相当于扔掉了(dropped out)，被扔掉的神经元在前馈和反馈中都不起作用。所以每当有一个新输入时，由于Dropout的随机性，相当于在训练一个新的网络，但是这些网络都共享参数。<br>这种策略减少了神经元之间的相互影响，强制网络学习出更鲁棒的features能整合多个随机神经元的输出而不依赖某一个特别的神经元。<br>测试的时候则不dropout了，但每个神经元输出值乘以0.5，作为引入了dropout后预测分布的均值的估计。</p>
<p>作者在前两个全连接层使用了dropout，同时dropout几乎倍增了网络收敛时间。</p>
<h1 id="5-具体训练过程"><a href="#5-具体训练过程" class="headerlink" title="5 具体训练过程"></a>5 具体训练过程</h1><p>使用Mini-batch stochastic gradient descent，batch_size=128, momentum=0.9, weight_decay=0.0005。</p>
<p>权重更新规则如下：<br><img src="/2017/04/17/paper-notes-alexnet/weight.jpg" alt="weight.jpg" title=""></p>
<p>其中i是迭代次数，v是动量变量(momentum variable), ε是学习率，学习率后面的则是整个batch对于w的平均梯度。</p>
<p>作者使用均值0标准差为0.01的高斯分布初始化权重，对于2、4、5和全连接层的bias初始值1（给ReLU单元提供正的输入加速早期学习），其他层bias初始值为0。</p>
<p>同时其在每一层使用相同学习率并在训练过程中手动调整：每当验证集错误率不再下降时就对学习率除以10.学习率初始值为0.1，并且最后训练完成时下降了3次。整个网络大概在120万张图片训练集上跑了90遍，使用两块GTX580 3G花了5到6天。</p>
<h1 id="6-结论"><a href="#6-结论" class="headerlink" title="6 结论"></a>6 结论</h1><ol>
<li><p>对最终正确率最关键的因素：网络深度（减一层2%、加一层1.7%）&gt; 单层容量（双GPU约1.7%）&gt; LRN（1.4%）&gt; PCA干扰（1%）&gt; 重叠Polling（0.4%）。即，虽然其他的Trick也有明显效果，但决定性的，还是尽可能保持一个足够深、足够大规模的网络。</p>
</li>
<li><p>文中除了常规的case展示，还把倒数第二层的欧氏距离用于衡量图片是否相似，展示了很多图片查询的例子。</p>
</li>
</ol>
]]></content>
      
        
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> PaperNotes </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[cmder中文显示相关问题解决方案(1.3以上版本)]]></title>
      <url>http://wentaoma.com/2016/08/31/cmder-chinese-encode/</url>
      <content type="html"><![CDATA[<p>cmder虽然Windows命令行的进阶版，虽然好看易用，但其中文编码一直是个问题。网上有不少博客给出解决方案，大部分都已因为版本更新失效。<br>本文解决方案针对1.3以上版本的cmder用户<br><a id="more"></a></p>
<h2 id="中文字体重叠问题"><a href="#中文字体重叠问题" class="headerlink" title="中文字体重叠问题"></a>中文字体重叠问题</h2><h3 id="错误方案："><a href="#错误方案：" class="headerlink" title="错误方案："></a>错误方案：</h3><blockquote>
<p>需要取消勾选设置中的<code>Monospace</code>选项</p>
</blockquote>
<h3 id="正确方案"><a href="#正确方案" class="headerlink" title="正确方案:"></a>正确方案:</h3><p>这个问题在<code>cmder v1.3.0</code>以上版本中已经修复，不需要进行任何操作</p>
<h2 id="ls命令中文路径-文件名乱码"><a href="#ls命令中文路径-文件名乱码" class="headerlink" title="ls命令中文路径/文件名乱码"></a><code>ls</code>命令中文路径/文件名乱码</h2><h3 id="错误方案：-1"><a href="#错误方案：-1" class="headerlink" title="错误方案："></a>错误方案：</h3><blockquote>
<p>添加4行命令到cmder/config/aliases文件末尾…</p>
</blockquote>
<p>在<code>cmder v1.3.0</code>以上版本初始创建的<code>cmder/config/user-aliases.cmd</code>文件中已经包含：</p>
<figure class="highlight bat"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ls=ls --show-control-chars -F --<span class="built_in">color</span> $*</div></pre></td></tr></table></figure>
<p>不需要添加其它命令（实际上添加了也没有效果）</p>
<h3 id="正确方案-1"><a href="#正确方案-1" class="headerlink" title="正确方案:"></a>正确方案:</h3><ol>
<li><p><code>win+ctrl+p</code>打开Settings</p>
</li>
<li><p>在Settings &gt; Startup &gt; Environment里添加：<code>set LANG=zh_CN.UTF8</code></p>
</li>
</ol>
<p><strong>PS：</strong></p>
<p>这样修改过<code>ls</code>可以正确显示中文，但<code>ls |more</code>还是会出现乱码。<br>如果改为<code>set LANG=zh_CN.GBK</code>可以解决这个问题。</p>
<p><strong>PPS：</strong></p>
<p>因为<code>cat</code>命令读取文件的编码与此有关，如果改为GBK则<code>cat</code>一个UTF8文件会显示乱码，<br>如果改为UTF8则<code>cat</code>一个GBK文件会显示乱码，<br>具体设置还是看你环境中常用编码。</p>
<h2 id="cd进一个中文目录，中文路径名显示乱码"><a href="#cd进一个中文目录，中文路径名显示乱码" class="headerlink" title="cd进一个中文目录，中文路径名显示乱码"></a><code>cd</code>进一个中文目录，中文路径名显示乱码</h2><p>经过上述设置，cmder一般情况下都能正常显示中文</p>
<p>但是如果你进入一个中文路径的话，cmder的路径的中文仍然是乱码<br>好在已经有人解决了这个问题</p>
<p><a href="https://github.com/cmderdev/cmder/pull/1070" target="_blank" rel="external">Parse the original prompt for cwd and env names by janschulz · Pull Request #1070 · cmderdev/cmder · GitHub</a></p>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>只需下载最新的Release（目前是1.3.1）：<br><a href="https://github.com/cmderdev/cmder/releases" target="_blank" rel="external">Releases · cmderdev/cmder · GitHub</a></p>
<p>然后解压覆盖就好</p>
<p>最后附上一张配置完成的示意图:</p>
<img src="/2016/08/31/cmder-chinese-encode/cmder-cn-encode.png" alt="cmder-cn-encode.png" title="">
]]></content>
      
        
        <tags>
            
            <tag> cmder </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[CSS选择器权重及样式优先级]]></title>
      <url>http://wentaoma.com/2016/08/19/CSS-selector-weight/</url>
      <content type="html"><![CDATA[<p>CSS样式应用到元素上的规则可以简述为：先将所有样式按来源、权重排序；然后取权重高的样式，权重相同的样式根据“就近原则”应用。<br>我们先从选择器权重说起：<br><a id="more"></a></p>
<h2 id="CSS选择器权重"><a href="#CSS选择器权重" class="headerlink" title="CSS选择器权重"></a>CSS选择器权重</h2><p>从高到低可以将CSS选择器权重排列如下：</p>
<p><strong>1. 内联样式<code>inline-style</code>权重最高，权值可以记为1000</strong></p>
<p><strong>2. ID选择器，权值记为100</strong></p>
<p><strong>3. Class选择器、属性选择器、伪类选择器，权值记为10</strong></p>
<p><strong>4. 元素、伪元素选择器，权值记为1</strong></p>
<p><strong>5. 通配符选择器<code>*</code>，权值为0(为什么记为0而不是无权重见下方)，结合符(空格、+，&gt;)不计入权重</strong></p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">h1</span> &#123;<span class="attribute">color</span>: red;&#125; <span class="comment">/*权重=0001*/</span></div><div class="line"><span class="selector-tag">p</span><span class="selector-class">.siderbar</span> &gt; <span class="selector-tag">em</span> &#123;<span class="attribute">color</span>: purple;&#125; <span class="comment">/*权重=0012*/</span></div><div class="line"><span class="selector-class">.clearfloat</span><span class="selector-pseudo">:after</span> &#123;<span class="attribute">content</span>:<span class="string">""</span>;<span class="attribute">display</span>:block;<span class="attribute">clear</span>:both;&#125; <span class="comment">/*权重=0020*/</span></div><div class="line"><span class="selector-tag">li</span><span class="selector-id">#answer</span> &#123;<span class="attribute">color</span>:navy;&#125; <span class="comment">/*权重=0101*/</span></div></pre></td></tr></table></figure>
<h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a><em>注意事项</em></h3><p><strong>1. 使用属性选择器选择ID，其权重仍为10，如：</strong></p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">ul</span> &gt; <span class="selector-tag">li</span><span class="selector-attr">[id="answer"]</span> &#123;<span class="attribute">color</span>:navy;&#125; <span class="comment">/*权重=0012*/</span></div><div class="line"><span class="selector-tag">li</span><span class="selector-id">#answer</span> &#123;<span class="attribute">color</span>:navy;&#125; <span class="comment">/*权重=0101*/</span></div></pre></td></tr></table></figure>
<p><strong>2. 权重值不会进位，即使有10个元素选择器，其权重也不如一个Class选择器权重高，即<code>000(10)&lt;0010</code></strong></p>
<p><strong>3. 拥有<code>!important</code>声明的样式声明其权重单独计算</strong></p>
<p>如果一个重要声明(有<code>!important</code>)权重和非重要声明(无<code>!important</code>)冲突，重要声明优先，如：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">h1</span> &#123;<span class="attribute">color</span>:red <span class="meta">!important</span>;&#125;</div><div class="line"><span class="selector-id">#title</span> &#123;<span class="attribute">color</span>:yellow <span class="meta">!important</span>;&#125;</div></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">h1</span> <span class="attr">id</span>=<span class="string">"title"</span> <span class="attr">style</span>=<span class="string">"color:black"</span>&gt;</span>Final Color is YELLOW<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></div></pre></td></tr></table></figure>
<p>两个重要声明，第二个<code>#title</code>权重更高，且都优先于内联非重要声明的样式。</p>
<p><strong>4. (默认)继承的样式没有权重，会被通配符样式(0权重)覆盖，如：</strong></p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">* &#123;<span class="attribute">color</span>:yellow;&#125;</div><div class="line"><span class="selector-tag">h1</span> &#123;<span class="attribute">color</span>:red;&#125;</div></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">h1</span>&gt;</span>RED and <span class="tag">&lt;<span class="name">em</span>&gt;</span>YELLOW<span class="tag">&lt;/<span class="name">em</span>&gt;</span><span class="tag">&lt;/<span class="name">h1</span>&gt;</span></div></pre></td></tr></table></figure>
<p><code>em</code>元素继承了<code>h1</code>的color样式，但是因为其无权重，所以会被<code>* {color:yellow;}</code>覆盖。<br>所以通配符选择器往往有一种短路继承的效果。</p>
<p>但是如果在样式中明确指定使用继承属性值，就要看选择器的权重来决定了。比如在上述CSS中加入<code>em{color:inherit;}</code><br><code>em</code>标签文本就显示为继承下来的红色。</p>
<iframe scrolling="no" width="100%" height="300" src="//jsfiddle.net/vcdvuv5p/embedded/css,html,result/light" frameborder="0" allowfullscreen></iframe>
<h2 id="CSS样式应用优先级分析"><a href="#CSS样式应用优先级分析" class="headerlink" title="CSS样式应用优先级分析"></a>CSS样式应用优先级分析</h2><p>了解了选择器权重之后，我们就可以来看一下CSS到底是怎么选择某个元素样式的：</p>
<ol>
<li><p><strong>列出每一条样式规则，每条规则都含有一个匹配给定元素的选择器</strong></p>
</li>
<li><p><strong>按照来源和重要声明对所有样式排序</strong></p>
<p> CSS的来源有三种：</p>
<ul>
<li><p>User Agent Stylesheet 用户代理的默认CSS（浏览器默认的CSS）</p>
</li>
<li><p>Author Stylesheet 开发人员定义的CSS</p>
</li>
<li><p>User Stylesheet 用户自定义的CSS</p>
<p>再考虑到重要声明，可以将样式如下排序（优先级从高到低）：</p>
</li>
</ul>
<ol>
<li><p>User Stylesheet (!important)</p>
</li>
<li><p>Author Stylesheet (!important)</p>
</li>
<li><p>Author Stylesheet (Normal)</p>
</li>
<li><p>User Stylesheet (Normal)</p>
</li>
<li><p>User Agent Stylesheet</p>
</li>
</ol>
</li>
<li><p><strong>按照选择器权重给所有样式声明排序</strong></p>
</li>
<li><p><strong>按照样式声明出现的顺序给其排序</strong></p>
<p> 在相同权重下，样式声明离被设置元素越近优先级别越高，即“就近原则”。</p>
<p> 由于这个原因，才有了推荐的链接样式排序(link-visited-hover-active, LVHA)</p>
 <figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="selector-pseudo">:link</span> &#123;<span class="attribute">color</span>:blue;&#125;</div><div class="line"><span class="selector-pseudo">:visited</span> &#123;<span class="attribute">color</span>:purple;&#125;</div><div class="line"><span class="selector-pseudo">:hover</span> &#123;<span class="attribute">color</span>:red;&#125;</div><div class="line"><span class="selector-pseudo">:active</span> &#123;<span class="attribute">color</span>:orange;&#125;</div></pre></td></tr></table></figure>
<p> 上述顺序如果换成：</p>
 <figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="selector-pseudo">:active</span> &#123;<span class="attribute">color</span>:orange;&#125;</div><div class="line"><span class="selector-pseudo">:hover</span> &#123;<span class="attribute">color</span>:red;&#125;</div><div class="line"><span class="selector-pseudo">:link</span> &#123;<span class="attribute">color</span>:blue;&#125;</div><div class="line"><span class="selector-pseudo">:visited</span> &#123;<span class="attribute">color</span>:purple;&#125;</div></pre></td></tr></table></figure>
<p> 则任何链接都不会显示<code>:hover</code>或<code>:active</code>样式。因为任何链接要么已访问，要么未访问，所以前两个样式会被覆盖。</p>
</li>
</ol>
]]></content>
      
        
        <tags>
            
            <tag> CSS </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Caffe-Python接口常用API参考]]></title>
      <url>http://wentaoma.com/2016/08/10/caffe-python-common-api-reference/</url>
      <content type="html"><![CDATA[<p>本文整理了pycaffe中常用的API<br><a id="more"></a></p>
<h2 id="Packages导入"><a href="#Packages导入" class="headerlink" title="Packages导入"></a>Packages导入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> caffe</div><div class="line"><span class="keyword">from</span> caffe <span class="keyword">import</span> layers <span class="keyword">as</span> L</div><div class="line"><span class="keyword">from</span> caffe <span class="keyword">import</span> params <span class="keyword">as</span> P</div></pre></td></tr></table></figure>
<h2 id="Layers定义"><a href="#Layers定义" class="headerlink" title="Layers定义"></a>Layers定义</h2><h3 id="Data层定义"><a href="#Data层定义" class="headerlink" title="Data层定义"></a>Data层定义</h3><h4 id="lmdb-leveldb-Data层定义"><a href="#lmdb-leveldb-Data层定义" class="headerlink" title="lmdb/leveldb Data层定义"></a>lmdb/leveldb Data层定义</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">L.Data( </div><div class="line">        source=lmdb,</div><div class="line">        backend=P.Data.LMDB,</div><div class="line">        batch_size=batch_size, ntop=<span class="number">2</span>,</div><div class="line">        transform_param=dict(</div><div class="line">                              crop_size=<span class="number">227</span>,</div><div class="line">                              mean_value=[<span class="number">104</span>, <span class="number">117</span>, <span class="number">123</span>],</div><div class="line">                              mirror=<span class="keyword">True</span></div><div class="line">                              )</div><div class="line">        )</div></pre></td></tr></table></figure>
<h4 id="HDF5-Data层定义"><a href="#HDF5-Data层定义" class="headerlink" title="HDF5 Data层定义"></a>HDF5 Data层定义</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">L.HDF5Data(</div><div class="line">            hdf5_data_param=&#123;</div><div class="line">                            <span class="string">'source'</span>: <span class="string">'./training_data_paths.txt'</span>,  </div><div class="line">                            <span class="string">'batch_size'</span>: <span class="number">64</span></div><div class="line">                            &#125;,</div><div class="line">            include=&#123;</div><div class="line">                    <span class="string">'phase'</span>: caffe.TRAIN</div><div class="line">                    &#125;</div><div class="line">            )</div></pre></td></tr></table></figure>
<h4 id="ImageData-Data层定义"><a href="#ImageData-Data层定义" class="headerlink" title="ImageData Data层定义"></a>ImageData Data层定义</h4><p>适用于txt文件一行记录一张图片的数据源</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">L.ImageData(</div><div class="line">                source=list_path,</div><div class="line">                batch_size=batch_size,</div><div class="line">                new_width=<span class="number">48</span>,</div><div class="line">                new_height=<span class="number">48</span>,</div><div class="line">                ntop=<span class="number">2</span>,</div><div class="line">                ransform_param=dict(crop_size=<span class="number">40</span>,mirror=<span class="keyword">True</span>)</div><div class="line">                )</div></pre></td></tr></table></figure>
<h3 id="Convloution层定义"><a href="#Convloution层定义" class="headerlink" title="Convloution层定义"></a>Convloution层定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">L.Convolution(  </div><div class="line">                bottom, </div><div class="line">                kernel_size=ks, </div><div class="line">                stride=stride,</div><div class="line">                num_output=nout, </div><div class="line">                pad=pad, </div><div class="line">                group=group</div><div class="line">                )</div></pre></td></tr></table></figure>
<h3 id="LRN层定义"><a href="#LRN层定义" class="headerlink" title="LRN层定义"></a>LRN层定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">L.LRN(</div><div class="line">        bottom, </div><div class="line">        local_size=<span class="number">5</span>, </div><div class="line">        alpha=<span class="number">1e-4</span>, </div><div class="line">        beta=<span class="number">0.75</span></div><div class="line">        )</div></pre></td></tr></table></figure>
<h3 id="Activation层定义"><a href="#Activation层定义" class="headerlink" title="Activation层定义"></a>Activation层定义</h3><h4 id="ReLU层定义"><a href="#ReLU层定义" class="headerlink" title="ReLU层定义"></a>ReLU层定义</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">L.ReLU(</div><div class="line">        bottom, </div><div class="line">        in_place=<span class="keyword">True</span></div><div class="line">        )</div></pre></td></tr></table></figure>
<h3 id="Pooling层定义"><a href="#Pooling层定义" class="headerlink" title="Pooling层定义"></a>Pooling层定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">L.Pooling(</div><div class="line">            bottom,</div><div class="line">            pool=P.Pooling.MAX, </div><div class="line">            kernel_size=ks, </div><div class="line">            stride=stride</div><div class="line">            )</div></pre></td></tr></table></figure>
<h3 id="FullConnect层定义"><a href="#FullConnect层定义" class="headerlink" title="FullConnect层定义"></a>FullConnect层定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">L.InnerProduct(</div><div class="line">                bottom, </div><div class="line">                num_output=nout</div><div class="line">                )</div></pre></td></tr></table></figure>
<h3 id="Dropout层定义"><a href="#Dropout层定义" class="headerlink" title="Dropout层定义"></a>Dropout层定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">L.Dropout(</div><div class="line">            bottom, </div><div class="line">            in_place=<span class="keyword">True</span></div><div class="line">            )</div></pre></td></tr></table></figure>
<h3 id="Loss层定义"><a href="#Loss层定义" class="headerlink" title="Loss层定义"></a>Loss层定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">L.SoftmaxWithLoss(</div><div class="line">                    bottom, </div><div class="line">                    label</div><div class="line">                    )</div></pre></td></tr></table></figure>
<h3 id="Accuracy层定义"><a href="#Accuracy层定义" class="headerlink" title="Accuracy层定义"></a>Accuracy层定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">L.Accuracy(</div><div class="line">            bottom,</div><div class="line">            label</div><div class="line">            )</div></pre></td></tr></table></figure>
<h3 id="转换为proto文本"><a href="#转换为proto文本" class="headerlink" title="转换为proto文本"></a>转换为proto文本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">caffe.to_proto(</div><div class="line">                loss, </div><div class="line">                acc     <span class="comment">#训练阶段可以删去Accuracy层</span></div><div class="line">                )</div></pre></td></tr></table></figure>
<h2 id="Solver定义"><a href="#Solver定义" class="headerlink" title="Solver定义"></a>Solver定义</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> caffe.proto <span class="keyword">import</span> caffe_pb2</div><div class="line"></div><div class="line">s = caffe_pb2.SolverParameter()</div><div class="line"></div><div class="line">path=<span class="string">'/home/xxx/data/'</span></div><div class="line">solver_file=path+<span class="string">'solver.prototxt'</span>     <span class="comment">#solver文件保存位置</span></div><div class="line"></div><div class="line">s.train_net = path+<span class="string">'train.prototxt'</span>     <span class="comment"># 训练配置文件</span></div><div class="line">s.test_net.append(path+<span class="string">'val.prototxt'</span>)  <span class="comment"># 测试配置文件</span></div><div class="line">s.test_interval = <span class="number">782</span>                   <span class="comment"># 测试间隔</span></div><div class="line">s.test_iter.append(<span class="number">313</span>)                 <span class="comment"># 测试迭代次数</span></div><div class="line">s.max_iter = <span class="number">78200</span>                      <span class="comment"># 最大迭代次数</span></div><div class="line"></div><div class="line">s.base_lr = <span class="number">0.001</span>                       <span class="comment"># 基础学习率</span></div><div class="line">s.momentum = <span class="number">0.9</span>                        <span class="comment"># momentum系数</span></div><div class="line">s.weight_decay = <span class="number">5e-4</span>                   <span class="comment"># 权值衰减系数</span></div><div class="line">s.lr_policy = <span class="string">'step'</span>                    <span class="comment"># 学习率衰减方法</span></div><div class="line">s.stepsize=<span class="number">26067</span>                        <span class="comment"># 此值仅对step方法有效</span></div><div class="line">s.gamma = <span class="number">0.1</span>                           <span class="comment"># 学习率衰减指数</span></div><div class="line">s.display = <span class="number">782</span>                         <span class="comment"># 屏幕日志显示间隔</span></div><div class="line">s.snapshot = <span class="number">7820</span></div><div class="line">s.snapshot_prefix = <span class="string">'shapshot'</span></div><div class="line">s.type = “SGD”                          <span class="comment"># 优化算法</span></div><div class="line">s.solver_mode = caffe_pb2.SolverParameter.GPU</div><div class="line"></div><div class="line"><span class="keyword">with</span> open(solver_file, <span class="string">'w'</span>) <span class="keyword">as</span> f:</div><div class="line">    f.write(str(s))</div></pre></td></tr></table></figure>
<h2 id="Model训练"><a href="#Model训练" class="headerlink" title="Model训练"></a>Model训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 训练设置</span></div><div class="line"><span class="comment"># 使用GPU</span></div><div class="line">caffe.set_device(gpu_id) <span class="comment"># 若不设置,默认为0</span></div><div class="line">caffe.set_mode_gpu()</div><div class="line"><span class="comment"># 使用CPU</span></div><div class="line">caffe.set_mode_cpu()</div><div class="line"></div><div class="line"><span class="comment"># 加载Solver，有两种常用方法</span></div><div class="line"><span class="comment"># 1. 无论模型中Slover类型是什么统一设置为SGD</span></div><div class="line">solver = caffe.SGDSolver(<span class="string">'/home/xxx/data/solver.prototxt'</span>) </div><div class="line"><span class="comment"># 2. 根据solver的prototxt中solver_type读取，默认为SGD</span></div><div class="line">solver = caffe.get_solver(<span class="string">'/home/xxx/data/solver.prototxt'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 训练模型</span></div><div class="line"><span class="comment"># 1.1 前向传播</span></div><div class="line">solver.net.forward()  <span class="comment"># train net</span></div><div class="line">solver.test_nets[<span class="number">0</span>].forward()  <span class="comment"># test net (there can be more than one)</span></div><div class="line"><span class="comment"># 1.2 反向传播,计算梯度</span></div><div class="line">solver.net.backward()</div><div class="line"><span class="comment"># 2. 进行一次前向传播一次反向传播并根据梯度更新参数</span></div><div class="line">solver.step(<span class="number">1</span>)</div><div class="line"><span class="comment"># 3. 根据solver文件中设置进行完整model训练</span></div><div class="line">solver.solve()</div></pre></td></tr></table></figure>
<p>如果想在训练过程中保存模型参数，调用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">solver.net.save(<span class="string">'mymodel.caffemodel'</span>)</div></pre></td></tr></table></figure>
<h2 id="分类图片"><a href="#分类图片" class="headerlink" title="分类图片"></a>分类图片</h2><h3 id="加载Model数据"><a href="#加载Model数据" class="headerlink" title="加载Model数据"></a>加载Model数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">net = caffe.Net(</div><div class="line">        deploy_prototxt_path,   <span class="comment"># 用于分类的网络定义文件路径</span></div><div class="line">        caffe_model_path,       <span class="comment"># 训练好模型路径</span></div><div class="line">        caffe.TEST              <span class="comment"># 设置为测试阶段</span></div><div class="line">        )</div></pre></td></tr></table></figure>
<h3 id="中值文件转换"><a href="#中值文件转换" class="headerlink" title="中值文件转换"></a>中值文件转换</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#　编写一个函数，将二进制的均值转换为python的均值</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_mean</span><span class="params">(binMean,npyMean)</span>:</span></div><div class="line">    blob = caffe.proto.caffe_pb2.BlobProto()</div><div class="line">    bin_mean = open(binMean, <span class="string">'rb'</span> ).read()</div><div class="line">    blob.ParseFromString(bin_mean)</div><div class="line">    arr = np.array( caffe.io.blobproto_to_array(blob) )</div><div class="line">    npy_mean = arr[<span class="number">0</span>]</div><div class="line">    np.save(npyMean, npy_mean )</div><div class="line"></div><div class="line"><span class="comment"># 调用函数转换均值</span></div><div class="line">binMean=<span class="string">'examples/cifar10/mean.binaryproto'</span></div><div class="line">npyMean=<span class="string">'examples/cifar10/mean.npy'</span></div><div class="line">convert_mean(binMean,npyMean)</div></pre></td></tr></table></figure>
<h3 id="图片预处理"><a href="#图片预处理" class="headerlink" title="图片预处理"></a>图片预处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 设定图片的shape格式为网络data层格式</span></div><div class="line">transformer = caffe.io.Transformer(&#123;<span class="string">'data'</span>: net.blobs[<span class="string">'data'</span>].data.shape&#125;)</div><div class="line"><span class="comment"># 改变维度的顺序，由原始图片维度(width, height, channel)变为(channel, width, height)</span></div><div class="line">transformer.set_transpose(<span class="string">'data'</span>, (<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>)) </div><div class="line"><span class="comment"># 减去均值,注意要先将binaryproto格式均值文件转换为npy格式[此步根据训练model时设置可选]</span></div><div class="line">transformer.set_mean(<span class="string">'data'</span>, np.load(mean_file_path).mean(<span class="number">1</span>).mean(<span class="number">1</span>))</div><div class="line"><span class="comment"># 缩放到[0，255]之间</span></div><div class="line">transformer.set_raw_scale(<span class="string">'data'</span>, <span class="number">255</span>)</div><div class="line"><span class="comment"># 交换通道，将图片由RGB变为BGR</span></div><div class="line">transformer.set_channel_swap(<span class="string">'data'</span>, (<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>))</div><div class="line"></div><div class="line"><span class="comment"># 加载图片</span></div><div class="line">im=caffe.io.load_image(img)</div><div class="line"></div><div class="line"><span class="comment"># 执行上面设置的图片预处理操作，并将图片载入到blob中</span></div><div class="line">net.blobs[<span class="string">'data'</span>].data[...] = transformer.preprocess(<span class="string">'data'</span>,im)</div></pre></td></tr></table></figure>
<h3 id="执行测试"><a href="#执行测试" class="headerlink" title="执行测试"></a>执行测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#执行测试</span></div><div class="line">out = net.forward()</div><div class="line"></div><div class="line">labels = np.loadtxt(labels_filename, str, delimiter=<span class="string">'\t'</span>)   <span class="comment">#读取类别名称文件</span></div><div class="line">prob= net.blobs[<span class="string">'Softmax1'</span>].data[<span class="number">0</span>].flatten() <span class="comment">#取出最后一层（Softmax）属于某个类别的概率值，并打印</span></div><div class="line"><span class="keyword">print</span> prob</div><div class="line">order=prob.argsort()[<span class="number">0</span>]  <span class="comment">#将概率值排序，取出最大值所在的序号 </span></div><div class="line"><span class="keyword">print</span> <span class="string">'the class is:'</span>,labels[order]   <span class="comment">#将该序号转换成对应的类别名称，并打印</span></div><div class="line"></div><div class="line"><span class="comment"># 取出前五个较大值所在的序号</span></div><div class="line">top_inds = prob.argsort()[::<span class="number">-1</span>][:<span class="number">5</span>]</div><div class="line"><span class="keyword">print</span> <span class="string">'probabilities and labels:'</span> zip(prob[top_inds], labels[top_inds])</div></pre></td></tr></table></figure>
<h3 id="各层信息显示"><a href="#各层信息显示" class="headerlink" title="各层信息显示"></a>各层信息显示</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># params显示：layer名，w，b</span></div><div class="line"><span class="keyword">for</span> layer_name, param <span class="keyword">in</span> net.params.items():</div><div class="line">    <span class="keyword">print</span> layer_name + <span class="string">'\t'</span> + str(param[<span class="number">0</span>].data.shape), str(param[<span class="number">1</span>].data.shape)</div><div class="line"></div><div class="line"><span class="comment"># blob显示：layer名，输出的blob维度</span></div><div class="line"><span class="keyword">for</span> layer_name, blob <span class="keyword">in</span> net.blobs.items():</div><div class="line">    <span class="keyword">print</span> layer_name + <span class="string">'\t'</span> + str(blob.data.shape)</div></pre></td></tr></table></figure>
<h3 id="自定义函数-参数-卷积结果可视化"><a href="#自定义函数-参数-卷积结果可视化" class="headerlink" title="自定义函数:参数/卷积结果可视化"></a>自定义函数:参数/卷积结果可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> matplotlib.image <span class="keyword">as</span> mpimg</div><div class="line"><span class="keyword">import</span> caffe</div><div class="line">%matplotlib inline</div><div class="line"></div><div class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">8</span>, <span class="number">8</span>)</div><div class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></div><div class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_data</span><span class="params">(data, padsize=<span class="number">1</span>, padval=<span class="number">0</span>)</span>:</span></div><div class="line"><span class="string">"""Take an array of shape (n, height, width) or (n, height, width, 3)</span></div><div class="line">       and visualize each (height, width) thing in a grid of size approx. sqrt(n) by sqrt(n)"""</div><div class="line">    <span class="comment"># data归一化</span></div><div class="line">    data -= data.min()</div><div class="line">    data /= data.max()</div><div class="line">    </div><div class="line">    <span class="comment"># 根据data中图片数量data.shape[0]，计算最后输出时每行每列图片数n</span></div><div class="line">    n = int(np.ceil(np.sqrt(data.shape[<span class="number">0</span>])))</div><div class="line">    <span class="comment"># padding = ((图片个数维度的padding),(图片高的padding), (图片宽的padding), ....)</span></div><div class="line">    padding = ((<span class="number">0</span>, n ** <span class="number">2</span> - data.shape[<span class="number">0</span>]), (<span class="number">0</span>, padsize), (<span class="number">0</span>, padsize)) + ((<span class="number">0</span>, <span class="number">0</span>),) * (data.ndim - <span class="number">3</span>)</div><div class="line">    data = np.pad(data, padding, mode=<span class="string">'constant'</span>, constant_values=(padval, padval))</div><div class="line">    </div><div class="line">    <span class="comment"># 先将padding后的data分成n*n张图像</span></div><div class="line">    data = data.reshape((n, n) + data.shape[<span class="number">1</span>:]).transpose((<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>) + tuple(range(<span class="number">4</span>, data.ndim + <span class="number">1</span>)))</div><div class="line">    <span class="comment"># 再将（n, W, n, H）变换成(n*w, n*H)</span></div><div class="line">    data = data.reshape((n * data.shape[<span class="number">1</span>], n * data.shape[<span class="number">3</span>]) + data.shape[<span class="number">4</span>:])</div><div class="line">    plt.figure()</div><div class="line">    plt.imshow(data,cmap=<span class="string">'gray'</span>)</div><div class="line">    plt.axis(<span class="string">'off'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 示例：显示第一个卷积层的输出数据和权值（filter）</span></div><div class="line"><span class="keyword">print</span> net.blobs[<span class="string">'conv1'</span>].data[<span class="number">0</span>].shape</div><div class="line">show_data(net.blobs[<span class="string">'conv1'</span>].data[<span class="number">0</span>])</div><div class="line"><span class="keyword">print</span> net.params[<span class="string">'conv1'</span>][<span class="number">0</span>].data.shape</div><div class="line">show_data(net.params[<span class="string">'conv1'</span>][<span class="number">0</span>].data.reshape(<span class="number">32</span>*<span class="number">3</span>,<span class="number">5</span>,<span class="number">5</span>))</div></pre></td></tr></table></figure>
<h3 id="自定义：训练过程Loss-amp-Accuracy可视化"><a href="#自定义：训练过程Loss-amp-Accuracy可视化" class="headerlink" title="自定义：训练过程Loss&amp;Accuracy可视化"></a>自定义：训练过程Loss&amp;Accuracy可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  </div><div class="line"><span class="keyword">import</span> caffe   </div><div class="line">caffe.set_device(<span class="number">0</span>)  </div><div class="line">caffe.set_mode_gpu()   </div><div class="line"><span class="comment"># 使用SGDSolver，即随机梯度下降算法  </span></div><div class="line">solver = caffe.SGDSolver(<span class="string">'/home/xxx/mnist/solver.prototxt'</span>)  </div><div class="line">  </div><div class="line"><span class="comment"># 等价于solver文件中的max_iter，即最大解算次数  </span></div><div class="line">niter = <span class="number">10000</span> </div><div class="line"></div><div class="line"><span class="comment"># 每隔100次收集一次loss数据  </span></div><div class="line">display= <span class="number">100</span>  </div><div class="line">  </div><div class="line"><span class="comment"># 每次测试进行100次解算 </span></div><div class="line">test_iter = <span class="number">100</span></div><div class="line"></div><div class="line"><span class="comment"># 每500次训练进行一次测试</span></div><div class="line">test_interval =<span class="number">500</span></div><div class="line">  </div><div class="line"><span class="comment">#初始化 </span></div><div class="line">train_loss = zeros(ceil(niter * <span class="number">1.0</span> / display))   </div><div class="line">test_loss = zeros(ceil(niter * <span class="number">1.0</span> / test_interval))  </div><div class="line">test_acc = zeros(ceil(niter * <span class="number">1.0</span> / test_interval))  </div><div class="line">  </div><div class="line"><span class="comment"># 辅助变量  </span></div><div class="line">_train_loss = <span class="number">0</span>; _test_loss = <span class="number">0</span>; _accuracy = <span class="number">0</span>  </div><div class="line"><span class="comment"># 进行解算  </span></div><div class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(niter):  </div><div class="line">    <span class="comment"># 进行一次解算  </span></div><div class="line">    solver.step(<span class="number">1</span>)  </div><div class="line">    <span class="comment"># 统计train loss  </span></div><div class="line">    _train_loss += solver.net.blobs[<span class="string">'SoftmaxWithLoss1'</span>].data  </div><div class="line">    <span class="keyword">if</span> it % display == <span class="number">0</span>:  </div><div class="line">        <span class="comment"># 计算平均train loss  </span></div><div class="line">        train_loss[it // display] = _train_loss / display  </div><div class="line">        _train_loss = <span class="number">0</span>  </div><div class="line">  </div><div class="line">    <span class="keyword">if</span> it % test_interval == <span class="number">0</span>:  </div><div class="line">        <span class="keyword">for</span> test_it <span class="keyword">in</span> range(test_iter):  </div><div class="line">            <span class="comment"># 进行一次测试  </span></div><div class="line">            solver.test_nets[<span class="number">0</span>].forward()  </div><div class="line">            <span class="comment"># 计算test loss  </span></div><div class="line">            _test_loss += solver.test_nets[<span class="number">0</span>].blobs[<span class="string">'SoftmaxWithLoss1'</span>].data  </div><div class="line">            <span class="comment"># 计算test accuracy  </span></div><div class="line">            _accuracy += solver.test_nets[<span class="number">0</span>].blobs[<span class="string">'Accuracy1'</span>].data  </div><div class="line">        <span class="comment"># 计算平均test loss  </span></div><div class="line">        test_loss[it / test_interval] = _test_loss / test_iter  </div><div class="line">        <span class="comment"># 计算平均test accuracy  </span></div><div class="line">        test_acc[it / test_interval] = _accuracy / test_iter  </div><div class="line">        _test_loss = <span class="number">0</span>  </div><div class="line">        _accuracy = <span class="number">0</span>  </div><div class="line">  </div><div class="line"><span class="comment"># 绘制train loss、test loss和accuracy曲线  </span></div><div class="line"><span class="keyword">print</span> <span class="string">'\nplot the train loss and test accuracy\n'</span>  </div><div class="line">_, ax1 = plt.subplots()  </div><div class="line">ax2 = ax1.twinx()  </div><div class="line">  </div><div class="line"><span class="comment"># train loss -&gt; 绿色  </span></div><div class="line">ax1.plot(display * arange(len(train_loss)), train_loss, <span class="string">'g'</span>)  </div><div class="line"><span class="comment"># test loss -&gt; 黄色  </span></div><div class="line">ax1.plot(test_interval * arange(len(test_loss)), test_loss, <span class="string">'y'</span>)  </div><div class="line"><span class="comment"># test accuracy -&gt; 红色  </span></div><div class="line">ax2.plot(test_interval * arange(len(test_acc)), test_acc, <span class="string">'r'</span>)  </div><div class="line">  </div><div class="line">ax1.set_xlabel(<span class="string">'iteration'</span>)  </div><div class="line">ax1.set_ylabel(<span class="string">'loss'</span>)  </div><div class="line">ax2.set_ylabel(<span class="string">'accuracy'</span>)  </div><div class="line">plt.show()</div></pre></td></tr></table></figure>]]></content>
      
        
        <tags>
            
            <tag> Caffe </tag>
            
            <tag> Python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[使用Caffe的Python接口进行Cifar10可视化]]></title>
      <url>http://wentaoma.com/2016/08/08/caffe-cifar10-model-visualization-with-python/</url>
      <content type="html"><![CDATA[<p>根据训练好的cifar10数据的model，从测试图片中选出一张进行测试，并进行网络模型、卷积结果及参数可视化<br>注意：本文中代码运行在windows+ipython notebook下，已事先配置好caffe的python接口<br><a id="more"></a></p>
<h2 id="导入必需的包"><a href="#导入必需的包" class="headerlink" title="导入必需的包"></a>导入必需的包</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> matplotlib.image <span class="keyword">as</span> mpimg</div><div class="line"><span class="keyword">import</span> caffe</div><div class="line">%matplotlib inline</div><div class="line"></div><div class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">8</span>, <span class="number">8</span>)</div><div class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></div><div class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></div></pre></td></tr></table></figure>
<h2 id="载入网络模型"><a href="#载入网络模型" class="headerlink" title="载入网络模型"></a>载入网络模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 载入模型，并显示各层数据信息</span></div><div class="line">caffe.set_mode_gpu()</div><div class="line">net = caffe.Net(<span class="string">'examples/cifar10/cifar10_quick.prototxt'</span>,</div><div class="line">                <span class="string">'examples/cifar10/cifar10_quick_iter_5000.caffemodel.h5'</span>,</div><div class="line">                caffe.TEST)</div><div class="line">[(k, v.data.shape) <span class="keyword">for</span> k, v <span class="keyword">in</span> net.blobs.items()]</div></pre></td></tr></table></figure>
<pre><code>[(&apos;data&apos;, (1L, 3L, 32L, 32L)),
 (&apos;conv1&apos;, (1L, 32L, 32L, 32L)),
 (&apos;pool1&apos;, (1L, 32L, 16L, 16L)),
 (&apos;conv2&apos;, (1L, 32L, 16L, 16L)),
 (&apos;pool2&apos;, (1L, 32L, 8L, 8L)),
 (&apos;conv3&apos;, (1L, 64L, 8L, 8L)),
 (&apos;pool3&apos;, (1L, 64L, 4L, 4L)),
 (&apos;ip1&apos;, (1L, 64L)),
 (&apos;ip2&apos;, (1L, 10L)),
 (&apos;prob&apos;, (1L, 10L))]
</code></pre><h2 id="可视化网络模型"><a href="#可视化网络模型" class="headerlink" title="可视化网络模型"></a>可视化网络模型</h2><p>使用GraphViz+Caffe的draw_net.py来可视化网络模型</p>
<figure class="highlight bat"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">Rem 运行以下命令前必需先安装配置GraphViz</span></div><div class="line"><span class="comment">Rem --rankdir参数为网络方向,BT代表图片上网络从底至顶绘出</span></div><div class="line">python ./Build/x64/Release/pycaffe/draw_net.py examples/cifar10/cifar10_quick_train_test.prototxt examples/cifar10/cifar-quick.png --rankdir=BT</div></pre></td></tr></table></figure>
<pre><code>Drawing net to examples/cifar10/cifar-quick.png
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#显示模型图片</span></div><div class="line">net_im = mpimg.imread(<span class="string">'examples/cifar10/cifar-quick.png'</span>)</div><div class="line">plt.imshow(net_im)</div><div class="line">plt.axis(<span class="string">'off'</span>)</div></pre></td></tr></table></figure>
<pre><code>(-0.5, 904.5, 2079.5, -0.5)
</code></pre><img src="/2016/08/08/caffe-cifar10-model-visualization-with-python/output_7_1.png" alt="output_7_1.png" title="">
<h2 id="加载测试图片"><a href="#加载测试图片" class="headerlink" title="加载测试图片"></a>加载测试图片</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#加载测试图片，并显示</span></div><div class="line">im = caffe.io.load_image(<span class="string">'examples/cifar10/cat.jpg'</span>)</div><div class="line"><span class="keyword">print</span> im.shape</div><div class="line">plt.imshow(im)</div><div class="line">plt.axis(<span class="string">'off'</span>)</div></pre></td></tr></table></figure>
<pre><code>(1200L, 1600L, 3L)

(-0.5, 1599.5, 1199.5, -0.5)
</code></pre><img src="/2016/08/08/caffe-cifar10-model-visualization-with-python/output_8_2.jpg" alt="output_8_2.jpg" title="">
<h2 id="转换均值"><a href="#转换均值" class="headerlink" title="转换均值"></a>转换均值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#　编写一个函数，将二进制的均值转换为python的均值</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_mean</span><span class="params">(binMean,npyMean)</span>:</span></div><div class="line">    blob = caffe.proto.caffe_pb2.BlobProto()</div><div class="line">    bin_mean = open(binMean, <span class="string">'rb'</span> ).read()</div><div class="line">    blob.ParseFromString(bin_mean)</div><div class="line">    arr = np.array( caffe.io.blobproto_to_array(blob) )</div><div class="line">    npy_mean = arr[<span class="number">0</span>]</div><div class="line">    np.save(npyMean, npy_mean )</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 调用函数转换均值</span></div><div class="line">binMean=<span class="string">'examples/cifar10/mean.binaryproto'</span></div><div class="line">npyMean=<span class="string">'examples/cifar10/mean.npy'</span></div><div class="line">convert_mean(binMean,npyMean)</div></pre></td></tr></table></figure>
<h2 id="将图片载入Blob"><a href="#将图片载入Blob" class="headerlink" title="将图片载入Blob"></a>将图片载入Blob</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#将图片载入blob中,并减去均值</span></div><div class="line">transformer = caffe.io.Transformer(&#123;<span class="string">'data'</span>: net.blobs[<span class="string">'data'</span>].data.shape&#125;)</div><div class="line">transformer.set_transpose(<span class="string">'data'</span>, (<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>))</div><div class="line">transformer.set_mean(<span class="string">'data'</span>, np.load(npyMean).mean(<span class="number">1</span>).mean(<span class="number">1</span>)) <span class="comment"># 减去均值</span></div><div class="line">transformer.set_raw_scale(<span class="string">'data'</span>, <span class="number">255</span>)  </div><div class="line">transformer.set_channel_swap(<span class="string">'data'</span>, (<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>))</div><div class="line">net.blobs[<span class="string">'data'</span>].data[...] = transformer.preprocess(<span class="string">'data'</span>,im)</div><div class="line">inputData=net.blobs[<span class="string">'data'</span>].data</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#显示减去均值前后的数据</span></div><div class="line">plt.figure()</div><div class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>),plt.title(<span class="string">"origin"</span>)</div><div class="line">plt.imshow(im)</div><div class="line">plt.axis(<span class="string">'off'</span>)</div><div class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>),plt.title(<span class="string">"subtract mean"</span>)</div><div class="line">plt.imshow(transformer.deprocess(<span class="string">'data'</span>, inputData[<span class="number">0</span>]))</div><div class="line">plt.axis(<span class="string">'off'</span>)</div></pre></td></tr></table></figure>
<pre><code>(-0.5, 31.5, 31.5, -0.5)
</code></pre><img src="/2016/08/08/caffe-cifar10-model-visualization-with-python/output_12_1.jpg" alt="output_12_1.jpg" title="">
<h2 id="编写用于参数-卷积结果可视化的函数"><a href="#编写用于参数-卷积结果可视化的函数" class="headerlink" title="编写用于参数/卷积结果可视化的函数"></a>编写用于参数/卷积结果可视化的函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#　编写一个函数，用于显示各层数据</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_data</span><span class="params">(data, padsize=<span class="number">1</span>, padval=<span class="number">0</span>)</span>:</span></div><div class="line">    <span class="comment"># data归一化</span></div><div class="line">    data -= data.min()</div><div class="line">    data /= data.max()</div><div class="line">    </div><div class="line">    <span class="comment"># 根据data中图片数量data.shape[0]，计算最后输出时每行每列图片数n</span></div><div class="line">    n = int(np.ceil(np.sqrt(data.shape[<span class="number">0</span>])))</div><div class="line">    <span class="comment"># padding = ((图片个数维度的padding),(图片高的padding), (图片宽的padding), ....)</span></div><div class="line">    padding = ((<span class="number">0</span>, n ** <span class="number">2</span> - data.shape[<span class="number">0</span>]), (<span class="number">0</span>, padsize), (<span class="number">0</span>, padsize)) + ((<span class="number">0</span>, <span class="number">0</span>),) * (data.ndim - <span class="number">3</span>)</div><div class="line">    data = np.pad(data, padding, mode=<span class="string">'constant'</span>, constant_values=(padval, padval))</div><div class="line">    </div><div class="line">    <span class="comment"># 先将padding后的data分成n*n张图像</span></div><div class="line">    data = data.reshape((n, n) + data.shape[<span class="number">1</span>:]).transpose((<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>) + tuple(range(<span class="number">4</span>, data.ndim + <span class="number">1</span>)))</div><div class="line">    <span class="comment"># 再将（n, W, n, H）变换成(n*w, n*H)</span></div><div class="line">    data = data.reshape((n * data.shape[<span class="number">1</span>], n * data.shape[<span class="number">3</span>]) + data.shape[<span class="number">4</span>:])</div><div class="line">    plt.figure()</div><div class="line">    plt.imshow(data,cmap=<span class="string">'gray'</span>)</div><div class="line">    plt.axis(<span class="string">'off'</span>)</div></pre></td></tr></table></figure>
<h2 id="可视化各层数据"><a href="#可视化各层数据" class="headerlink" title="可视化各层数据"></a>可视化各层数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 运行模型并显示第一个卷积层的输出数据和权值（filter）</span></div><div class="line">net.forward()</div><div class="line"><span class="keyword">print</span> net.blobs[<span class="string">'conv1'</span>].data[<span class="number">0</span>].shape</div><div class="line">show_data(net.blobs[<span class="string">'conv1'</span>].data[<span class="number">0</span>])</div><div class="line"><span class="keyword">print</span> net.params[<span class="string">'conv1'</span>][<span class="number">0</span>].data.shape</div><div class="line">show_data(net.params[<span class="string">'conv1'</span>][<span class="number">0</span>].data.reshape(<span class="number">32</span>*<span class="number">3</span>,<span class="number">5</span>,<span class="number">5</span>))</div></pre></td></tr></table></figure>
<pre><code>(32L, 32L, 32L)
(32L, 3L, 5L, 5L)
</code></pre><img src="/2016/08/08/caffe-cifar10-model-visualization-with-python/output_14_1.png" alt="output_14_1.png" title="">
<img src="/2016/08/08/caffe-cifar10-model-visualization-with-python/output_14_2.png" alt="output_14_2.png" title="">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 显示第一次pooling后的输出数据</span></div><div class="line">show_data(net.blobs[<span class="string">'pool1'</span>].data[<span class="number">0</span>])</div><div class="line">net.blobs[<span class="string">'pool1'</span>].data.shape</div></pre></td></tr></table></figure>
<pre><code>(1L, 32L, 16L, 16L)
</code></pre><img src="/2016/08/08/caffe-cifar10-model-visualization-with-python/output_15_1.png" alt="output_15_1.png" title="">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 显示第二次卷积后的输出数据以及相应的权值（filter）</span></div><div class="line">show_data(net.blobs[<span class="string">'conv2'</span>].data[<span class="number">0</span>],padval=<span class="number">0.5</span>)</div><div class="line"><span class="keyword">print</span> net.blobs[<span class="string">'conv2'</span>].data.shape</div><div class="line">show_data(net.params[<span class="string">'conv2'</span>][<span class="number">0</span>].data.reshape(<span class="number">32</span>**<span class="number">2</span>,<span class="number">5</span>,<span class="number">5</span>))</div><div class="line"><span class="keyword">print</span> net.params[<span class="string">'conv2'</span>][<span class="number">0</span>].data.shape</div></pre></td></tr></table></figure>
<pre><code>(1L, 32L, 16L, 16L)
(32L, 32L, 5L, 5L)
</code></pre><img src="/2016/08/08/caffe-cifar10-model-visualization-with-python/output_16_1.png" alt="output_16_1.png" title="">
<img src="/2016/08/08/caffe-cifar10-model-visualization-with-python/output_16_2.png" alt="output_16_2.png" title="">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 显示第三次卷积后的输出数据以及相应的权值（filter）,取前１024个进行显示</span></div><div class="line">show_data(net.blobs[<span class="string">'conv3'</span>].data[<span class="number">0</span>],padval=<span class="number">0.5</span>)</div><div class="line"><span class="keyword">print</span> net.blobs[<span class="string">'conv3'</span>].data.shape</div><div class="line">show_data(net.params[<span class="string">'conv3'</span>][<span class="number">0</span>].data.reshape(<span class="number">64</span>*<span class="number">32</span>,<span class="number">5</span>,<span class="number">5</span>)[:<span class="number">1024</span>])</div><div class="line"><span class="keyword">print</span> net.params[<span class="string">'conv3'</span>][<span class="number">0</span>].data.shape</div></pre></td></tr></table></figure>
<pre><code>(1L, 64L, 8L, 8L)
(64L, 32L, 5L, 5L)
</code></pre><img src="/2016/08/08/caffe-cifar10-model-visualization-with-python/output_17_1.png" alt="output_17_1.png" title="">
<img src="/2016/08/08/caffe-cifar10-model-visualization-with-python/output_17_2.png" alt="output_17_2.png" title="">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 显示第三次池化后的输出数据</span></div><div class="line">show_data(net.blobs[<span class="string">'pool3'</span>].data[<span class="number">0</span>],padval=<span class="number">0.2</span>)</div><div class="line"><span class="keyword">print</span> net.blobs[<span class="string">'pool3'</span>].data.shape</div></pre></td></tr></table></figure>
<pre><code>(1L, 64L, 4L, 4L)
</code></pre><img src="/2016/08/08/caffe-cifar10-model-visualization-with-python/output_18_1.png" alt="output_18_1.png" title="">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 最后一层输入属于某个类的概率</span></div><div class="line">feat = net.blobs[<span class="string">'prob'</span>].data[<span class="number">0</span>]</div><div class="line"><span class="keyword">print</span> feat</div><div class="line">plt.plot(feat.flat)</div></pre></td></tr></table></figure>
<pre><code>[ 0.00170287  0.00115923  0.0225699   0.60395384  0.00453733  0.14171894
  0.00307363  0.01260873  0.15008588  0.05858969]

[&lt;matplotlib.lines.Line2D at 0x3bd38080&gt;]
</code></pre>
<p>与cifar10中的10种类型名称进行对比：</p>
<p>airplane、automobile、bird、cat、deer、dog、frog、horse、ship、truck</p>
<p>根据测试结果，判断为Cat。</p>
]]></content>
      
        
        <tags>
            
            <tag> Caffe </tag>
            
            <tag> Python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[JS函数作用域链]]></title>
      <url>http://wentaoma.com/2016/08/05/JS-function-scope-chain/</url>
      <content type="html"><![CDATA[<p>本文分析JS的函数作用域链，并从此角度理解闭包</p>
<p>关于JS作用域，请阅读<a href="http://yanhaijing.com/javascript/2014/04/30/JavaScript-Scoping-and-Hoisting/" target="_blank" rel="external">JavaScript的作用域和提升机制</a><br><a id="more"></a></p>
<h2 id="函数作用域链"><a href="#函数作用域链" class="headerlink" title="函数作用域链"></a>函数作用域链</h2><p>当某个函数第一次被调用时，会创建一个执行环境(execution context)以及相应作用域链(scope chain)。<br>然后使用this, arguments和函数中其它命名参数的值来初始化函数的活动对象(activation object)做变量对象使用并放在<br>作用域链的首位。而作用域链的终点则是全局执行环境的变量对象。</p>
<p>对于每一个执行环境而言，都有一个表示(自己作用域中)变量(引用)的对象，称为变量对象。全局环境中变量对象<br>始终存在，而其它函数的变量对象只在执行过程中以活动对象形式存在。</p>
<p>在函数执行过程中，就按作用域链的顺序查找变量。</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">compare</span>(<span class="params">value1, value2</span>)</span>&#123;</div><div class="line">    <span class="keyword">if</span>(value1 &lt; value2)&#123;</div><div class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</div><div class="line">    &#125;<span class="keyword">else</span> <span class="keyword">if</span>(value1 &gt; value2)&#123;</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span>;</div><div class="line">    &#125;<span class="keyword">else</span>&#123;</div><div class="line">        <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">var</span> result = compare(<span class="number">5</span>, <span class="number">10</span>);</div></pre></td></tr></table></figure>
<p>上述代码在调用<code>compare(5, 10)</code>时，<code>compare</code>函数执行环境与作用域链如下图所示：</p>
<img src="/2016/08/05/JS-function-scope-chain/js-function-scope-chain.png" alt="js-function-scope-chain.png" title="">
<p>作用域链本质其实是一个指向变量对象的指针列表。</p>
<p>实际上，在创建(声明)函数时，会预先创建一个包含全局变量对象的作用域链，保存在内部[[scope]]属性中。<br>在调用函数时，将[[scope]]中内容复制到函数执行环境作用域链中，并在作用域链前端添加当前函数活动对象。</p>
<p>一般来说当函数执行完毕后其活动对象就会被销毁，内存中仅保存全局变量对象。但是闭包情况则是例外。</p>
<h2 id="函数作用域链与闭包"><a href="#函数作用域链与闭包" class="headerlink" title="函数作用域链与闭包"></a>函数作用域链与闭包</h2><p>在一个函数内部定义的函数，在其创建(即外部函数执行)时，会将外部函数的活动对象添加到自己的作用域链[[scope]]中。<br>从而可以访问外部函数中定义的所有变量。<br>如果外部函数，这个内部函数被返回，因为内部函数作用域链仍然引用着外部函数活动对象，所以虽然外部函数在执行完毕后，<br>执行环境的作用域链会被销毁，但其活动对象仍然留在内存中。</p>
<p>这种携带包含它的函数的作用域的函数也就是闭包。</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">createComparisonFunction</span>(<span class="params">propertyName</span>)</span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">return</span> <span class="function"><span class="keyword">function</span>(<span class="params">object1, object2</span>)</span>&#123;</div><div class="line">        <span class="keyword">var</span> value1 = object1[propertyName];</div><div class="line">        <span class="keyword">var</span> value2 = object2[propertyName];</div><div class="line"></div><div class="line">        <span class="keyword">if</span>(value1 &lt; value2)&#123;</div><div class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</div><div class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(value1 &gt; value2)&#123;</div><div class="line">            <span class="keyword">return</span> <span class="number">1</span>;</div><div class="line">        &#125;<span class="keyword">else</span>&#123;</div><div class="line">            <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">        &#125;</div><div class="line">    &#125;;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//创建函数(闭包)</span></div><div class="line"><span class="keyword">var</span> compareNames = createComparisonFunction(<span class="string">"name"</span>);</div><div class="line"></div><div class="line"><span class="comment">//调用函数</span></div><div class="line"><span class="keyword">var</span> result = compareNames(&#123;<span class="attr">name</span>: <span class="string">"Amy"</span>&#125;, &#123;<span class="attr">name</span>: <span class="string">"Bob"</span>&#125;);</div><div class="line"></div><div class="line"><span class="comment">//解除引用，以便释放内存</span></div><div class="line">compareNames = <span class="literal">null</span>;</div></pre></td></tr></table></figure>
<p>上面代码中<code>compareNames</code>引用的匿名函数执行环境作用域链如下图所示：</p>
<img src="/2016/08/05/JS-function-scope-chain/js-closure-scope-chain.png" alt="js-closure-scope-chain.png" title="">
<h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><h4 id="1-闭包与变量"><a href="#1-闭包与变量" class="headerlink" title="1. 闭包与变量"></a>1. 闭包与变量</h4><p>闭包引用的外部函数的整个变量对象(活动对象)，(变量对象)其中保存着对函数内部所有变量的引用而不是变量的值，因而<br>其只能取得外部函数中变量在执行完后的最后一个值。</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">createFunctions</span>(<span class="params"></span>)</span>&#123;</div><div class="line">    <span class="keyword">var</span> result = [];</div><div class="line"></div><div class="line">    <span class="keyword">for</span>(<span class="keyword">var</span> i=<span class="number">0</span>; i&lt;<span class="number">10</span>, i++)&#123;</div><div class="line">        result[i] = <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</div><div class="line">            <span class="keyword">return</span> i;</div><div class="line">        &#125;;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">return</span> result;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>因为这一点，上述函数返回的函数数组中每一个调用返回的i值都为10.</p>
<h4 id="2-this变量"><a href="#2-this变量" class="headerlink" title="2. this变量"></a>2. this变量</h4><p>每个函数被调用时，其活动对象都会自动取得两个特殊变量：<code>this</code>,<code>arguments</code>。因此内部函数在作用域链上<br>搜索这两个变量时永远也不可能直接访问到外部函数中的这两个变量。不过可以通过将this保存在一个内部函数能够访问的<br>变量里，让其访问到该对象。</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">var</span> name = <span class="string">"Window"</span></div><div class="line"></div><div class="line"><span class="keyword">var</span> object = &#123;</div><div class="line">    <span class="attr">name</span>: <span class="string">"Object"</span>,</div><div class="line">    <span class="attr">getNameFunction</span>: <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</div><div class="line">        <span class="keyword">return</span> <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</div><div class="line">            <span class="keyword">return</span> <span class="keyword">this</span>.name;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">alert(object.getNameFunction()()); <span class="comment">//"Window"</span></div></pre></td></tr></table></figure>
<h4 id="3-内存泄漏"><a href="#3-内存泄漏" class="headerlink" title="3. 内存泄漏"></a>3. 内存泄漏</h4><p>IE9以前版本对JS对象和DOM对象使用不同垃圾回收例程。在这些IE版本中如果闭包作用域链保存这HTML元素，<br>则该元素无法被正确回收。</p>
<h2 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h2><p><a href="http://wwsun.github.io/posts/scope-and-context-in-javascript.html" target="_blank" rel="external">理解JavaScript中的作用域和上下文</a></p>
]]></content>
      
        
        <tags>
            
            <tag> JavaScript </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[JS继承方式总结]]></title>
      <url>http://wentaoma.com/2016/08/04/JS-inherit-conclusion/</url>
      <content type="html"><![CDATA[<p>根据《JS高级程序设计·第三版》第六章内容，总结了下JS的继承方式<br><a id="more"></a></p>
<h1 id="Part1-理解Prototype原型对象"><a href="#Part1-理解Prototype原型对象" class="headerlink" title="Part1. 理解Prototype原型对象"></a>Part1. 理解Prototype原型对象</h1><p>无论什么时候只要创建了一个新函数，JS就会为该函数创建一个prototype属性指向此函数原型对象。<br>同时，原型对象会自动获得一个constructor属性，指向prototype属性所在函数指针。<br>当创建一个构造函数，并调用该构造函数创建一个新实例后，实例内部也将有一个指针（ES5中叫做[[Prototype]]），<br>Firefox，Safari，Chrome上为__proto__属性，其指向构造函数的原型对象。<br>其在原型对象上定义的属性为所有对象实例共享，称为原型属性。</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">Person</span>(<span class="params"></span>)</span>&#123;</div><div class="line">&#125;</div><div class="line"></div><div class="line">Person.prototype.name = <span class="string">"Nicholas"</span>;</div><div class="line">Person.prototype.sayName = <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</div><div class="line">    alert(<span class="keyword">this</span>.name);</div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="keyword">var</span> person1 = <span class="keyword">new</span> Person();</div><div class="line"><span class="keyword">var</span> person2 = <span class="keyword">new</span> Person();</div></pre></td></tr></table></figure>
<p>上述代码的实例、构造函数、和原型对象之间关系如下图所示：</p>
<img src="/2016/08/04/JS-inherit-conclusion/js_understand_prototype.png" alt="js_understand_prototype.png" title="">
<h1 id="Part2-JS继承方式总结"><a href="#Part2-JS继承方式总结" class="headerlink" title="Part2. JS继承方式总结"></a>Part2. JS继承方式总结</h1><h2 id="原型链继承"><a href="#原型链继承" class="headerlink" title="原型链继承"></a>原型链继承</h2><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">SuperType</span>(<span class="params"></span>)</span>&#123;</div><div class="line">    <span class="keyword">this</span>.property = <span class="literal">true</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line">SuperType.prototype.getSuperValue = <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">this</span>.property;</div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">SubType</span>(<span class="params"></span>)</span>&#123;</div><div class="line">    <span class="keyword">this</span>.subproperty = <span class="literal">false</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//直接重写prototype对象实现继承SuperType</span></div><div class="line">SubType.prototype = <span class="keyword">new</span> SuperType();</div><div class="line"></div><div class="line">Sub.prototype.getSUbValue = <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">this</span>.subproperty;</div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="keyword">var</span> instance = <span class="keyword">new</span> SubType();</div><div class="line">alert(instance.getSuperValue()); <span class="comment">//true</span></div></pre></td></tr></table></figure>
<p>上述代码的继承是通过以一个<code>SuperType</code>实例替换<code>SubType</code>默认原型实现。<br>要注意此时：<code>instance.constructor</code>指向<code>SuperType</code>.</p>
<h3 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h3><ol>
<li><p>父类中实例属性成为子类原型属性，被子类所有实例共享。</p>
</li>
<li><p>创建子类型实例时，不能向父类型构造函数传递参数。</p>
</li>
</ol>
<h2 id="借用构造函数继承-经典-伪造对象继承"><a href="#借用构造函数继承-经典-伪造对象继承" class="headerlink" title="借用构造函数继承(经典/伪造对象继承)"></a>借用构造函数继承(经典/伪造对象继承)</h2><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">SuperType</span>(<span class="params">name</span>)</span>&#123;</div><div class="line">    <span class="keyword">this</span>.name = name;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">SubType</span>(<span class="params"></span>)</span>&#123;</div><div class="line">    <span class="comment">//调用父类构造函数实现继承，并可以传递参数</span></div><div class="line">    SuperType.call(<span class="keyword">this</span>, <span class="string">"Nicholas"</span>);</div><div class="line"></div><div class="line">    <span class="comment">//再添加子类实例属性、方法</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">var</span> instance = <span class="keyword">new</span> SubType();</div><div class="line">alert(instance.name); <span class="comment">//"Nicholas"</span></div></pre></td></tr></table></figure>
<p>借用构造函数继承是通过在子类型构造函数内部使用apply或call调用父类型构造函数，<br>从而在新创建对象上执行父类构造函数来实现。其可以向父类型构造函数传递参数。</p>
<h3 id="存在的问题-1"><a href="#存在的问题-1" class="headerlink" title="存在的问题"></a>存在的问题</h3><p>父类型原型中定义的方法对子类型不可见。</p>
<h2 id="组合继承"><a href="#组合继承" class="headerlink" title="组合继承"></a>组合继承</h2><p>也称为伪经典继承，将原型链和借用构造函数继承组合起来使用，是最常用的继承方法。</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">SuperType</span>(<span class="params">name</span>)</span>&#123;</div><div class="line">    <span class="keyword">this</span>.name = name;</div><div class="line">&#125;</div><div class="line"></div><div class="line">SuperType.prototype.sayName = <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</div><div class="line">    alert(<span class="keyword">this</span>.name);</div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">SubType</span>(<span class="params">name, age</span>)</span>&#123;</div><div class="line">    <span class="comment">//借用构造函数继承实例属性</span></div><div class="line">    SuperType.call(<span class="keyword">this</span>, name);</div><div class="line"></div><div class="line">    <span class="keyword">this</span>.age = age;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//原型链继承原型方法</span></div><div class="line">SubType.prototype = <span class="keyword">new</span> SuperType();</div></pre></td></tr></table></figure>
<h3 id="原型式继承"><a href="#原型式继承" class="headerlink" title="原型式继承"></a>原型式继承</h3><p>不需要额外创建新的构造函数，让新对象与原对象保持类似，但新对象会共享原对象的所有属性。</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">object</span>(<span class="params">o</span>)</span>&#123;</div><div class="line">    <span class="function"><span class="keyword">function</span> <span class="title">F</span>(<span class="params"></span>)</span>&#123;&#125; <span class="comment">//创建一个临时性构造函数</span></div><div class="line">    F.prototype = o; <span class="comment">//设置原型</span></div><div class="line">    <span class="keyword">return</span> <span class="keyword">new</span> F(); <span class="comment">//返回临时类型实例</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">var</span> person = &#123;</div><div class="line">    <span class="attr">name</span>: <span class="string">"Nicholas"</span>,</div><div class="line">    <span class="attr">friends</span>: [<span class="string">"Amy"</span>, <span class="string">"Bruno"</span>, <span class="string">"Candy"</span>]</div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="keyword">var</span> subPerson1 = object(person);</div><div class="line">subPerson1.friends.push(<span class="string">"Danel"</span>);</div><div class="line"></div><div class="line"><span class="keyword">var</span> subPerson2 = object(person);</div><div class="line">subPerson2.friends.push(<span class="string">"Ely"</span>);</div><div class="line"></div><div class="line"><span class="comment">//因为subPerson1, subPerson2共享同一个原型对象，所以：</span></div><div class="line">alert(subPerson2.friends); <span class="comment">//Amy, Bruno, Candy, Danel, Ely</span></div><div class="line">alert(Person.friends); <span class="comment">//Amy, Bruno, Candy, Danel, Ely</span></div></pre></td></tr></table></figure>
<p>ES5中<code>Object.create()</code>方法规范化了原型式继承。<br>其接收两个参数:</p>
<ol>
<li><p>用做新对象原型的对象</p>
</li>
<li><p>[可选]为新对象定义额外属性的对象</p>
</li>
</ol>
<h3 id="寄生式继承"><a href="#寄生式继承" class="headerlink" title="寄生式继承"></a>寄生式继承</h3><p>即创建一个仅用于封装继承过程的函数，该函数在函数内部以某种方式增强对象，最后返回此对象。<br>因为其仅在原对象基础上增强而不是自定义类型和构造函数，故称为寄生式继承。</p>
<p>但其不能做到函数复用。</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">object</span>(<span class="params">o</span>)</span>&#123;</div><div class="line">    <span class="function"><span class="keyword">function</span> <span class="title">F</span>(<span class="params"></span>)</span>&#123;&#125;</div><div class="line">    F.prototype = o;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">new</span> F();</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">createAnother</span>(<span class="params">original</span>)</span>&#123;</div><div class="line">    <span class="keyword">var</span> clone = object(original); <span class="comment">//可以使用任何返回新对象的函数</span></div><div class="line">    clone.sayHi = <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;     <span class="comment">//增强返回对象</span></div><div class="line">        alert(<span class="string">"Hi"</span>);</div><div class="line">    &#125;;</div><div class="line">    <span class="keyword">return</span> clone; <span class="comment">//返回增强对象</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">var</span> person = &#123;</div><div class="line">    <span class="attr">name</span>: <span class="string">"Nicholas"</span></div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="keyword">var</span> anotherPerson = createAnother(person);</div></pre></td></tr></table></figure>
<h3 id="寄生组合式继承"><a href="#寄生组合式继承" class="headerlink" title="寄生组合式继承"></a>寄生组合式继承</h3><p>前面说过，组合继承是JS最常用的继承模式，但其也存在一个问题：</p>
<p><em><em>无论什么情况下都会调用两次父类构造函数</em></em></p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">SuperType</span>(<span class="params">name</span>)</span>&#123;</div><div class="line">    <span class="keyword">this</span>.name = name;</div><div class="line">&#125;</div><div class="line"></div><div class="line">SuperType.prototype.sayName = <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</div><div class="line">    alert(<span class="keyword">this</span>.name);</div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">SubType</span>(<span class="params">name, age</span>)</span>&#123;</div><div class="line">    SuperType.call(<span class="keyword">this</span>, name);<span class="comment">//第二次调用</span></div><div class="line"></div><div class="line">    <span class="keyword">this</span>.age = age;</div><div class="line">&#125;</div><div class="line"></div><div class="line">SubType.prototype = <span class="keyword">new</span> SuperType();<span class="comment">//第一次调用</span></div></pre></td></tr></table></figure>
<p>这样导致我们在子类的prototype上创建了无用的SuperType的实例属性，<br>因为所有的父类实例属性都会在第二次使用<code>call</code>调用时被覆盖</p>
<p>对于这个问题的解决办法就是：寄生组合式继承</p>
<p>其和组合式继承区别在于，就是采用寄生方式去用父类型的prototype去覆盖子类型的prototype。<br>而不是使用父类型的实例去重设子类型的prototype。</p>
<p>因为我们仅仅需要在父类型prototype上的原型属性，而父类型的实例属性则之后通过借用构造函数方式继承。</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">inheritPrototype</span>(<span class="params">subType, superType</span>)</span>&#123;</div><div class="line">    <span class="keyword">var</span> prototype = object(superType.prototype); <span class="comment">//创建父类型"副本"</span></div><div class="line">    prototype.constructor = subType; <span class="comment">//重设constructor属性</span></div><div class="line">    subType.prototype = prototype; <span class="comment">//设置子类型prototype</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">SuperType</span>(<span class="params">name</span>)</span>&#123;</div><div class="line">    <span class="keyword">this</span>.name = name;</div><div class="line">&#125;</div><div class="line"></div><div class="line">SuperType.prototype.sayName = <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</div><div class="line">    alert(<span class="keyword">this</span>.name);</div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">SubType</span>(<span class="params">name, age</span>)</span>&#123;</div><div class="line">    SuperType.call(<span class="keyword">this</span>, name); <span class="comment">//继承父类实例属性</span></div><div class="line"></div><div class="line">    <span class="keyword">this</span>.age = age;</div><div class="line">&#125;</div><div class="line"></div><div class="line">inheritPrototype(SubType, SuperType); <span class="comment">//继承父类原型属性</span></div></pre></td></tr></table></figure>
]]></content>
      
        
        <tags>
            
            <tag> JavaScript </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Hello World, Hexo & NexT]]></title>
      <url>http://wentaoma.com/2016/07/31/hello-world/</url>
      <content type="html"><![CDATA[<p>Hexo &amp; NexT Theme Quick Start.<br><a id="more"></a></p>
<h2 id="Hexo-Quick-Start"><a href="#Hexo-Quick-Start" class="headerlink" title="Hexo Quick Start"></a>Hexo Quick Start</h2><p>Check <a href="https://hexo.io/docs/" target="_blank" rel="external">Hexo documentation</a> for more info.</p>
<h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
<h2 id="NexT-Quick-Start"><a href="#NexT-Quick-Start" class="headerlink" title="NexT Quick Start"></a>NexT Quick Start</h2><p>请查看<a href="http://theme-next.iissnan.com/getting-started.html" target="_blank" rel="external">中文文档</a>。</p>
<h2 id="Useful-Links"><a href="#Useful-Links" class="headerlink" title="Useful Links"></a>Useful Links</h2><ul>
<li><p><a href="http://www.arao.me/2015/hexo-next-theme-optimize-base/" target="_blank" rel="external">动动手指，NexT主题与Hexo更搭哦（基础篇）</a></p>
</li>
<li><p><a href="http://theme-next.iissnan.com/faqs.html" target="_blank" rel="external">NexT中文FAQ</a></p>
</li>
<li><p><a href="http://www.selfrebuild.net/2015/06/24/Github-Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/" target="_blank" rel="external">Github+Hexo搭建博客教程</a></p>
</li>
</ul>
]]></content>
      
        
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
