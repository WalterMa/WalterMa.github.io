<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[ImageNet Classification with Deep ConvolutionalNeural Networks - 论文笔记]]></title>
      <url>http://wentaoma.com/2017/04/17/paper-notes-alexnet/</url>
      <content type="html"><![CDATA[<p>原文地址:<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="external">ImageNet Classification with Deep ConvolutionalNeural Networks</a></p>
<p>AlexNet是2012年ILSVRC的冠军, Top1和Top5的Error Rate分别为: 37.5%, 17.0%.<br><a id="more"></a></p>
<h1 id="1-__u7B80_u4ECB"><a href="#1-__u7B80_u4ECB" class="headerlink" title="1. 简介"></a>1. 简介</h1><p>在此论文发表时， 基于已标记的小型图片数据集(如：NORB, Caltech-101/256, and CIFAR-10/100)的简单物体识别工作已经取得了相当优异的结果。例如MNIST数字识别的当前(即论文发表时间:2012)最优异的Error Rate(&lt;0.3%)已经超越了人类的水平。<br>然而真实情况下的物体识别要更复杂，同时也需要更大的数据集。但是这种大量的已标注的数据集直到最近才有收集到的可能，包括LabelMe和ImageNet等。</p>
<p>为了从大量图片数据中学习，需要一个具有巨大Learning Capacity的模型。同时因为物体识别的复杂性，其问题即使诸如ImageNet这种量级的数据集也不可能完全指定，因此选用的模型也必须有较好的泛化能力。卷积神经网络(CNN)则是满足这些要求的一种模型。CNN的Capacity可以由它的的深度和广度来控制，同时对于自然图像其也能给出强且几乎正确的预测。另外相对于拥有近似层数的标准前馈神经网络，CNN由于更少的连接数和参数个数因而更容易训练同时只会略微降低理论最佳表现。</p>
<p>尽管CNN有如此之多的好处，但将其大规模用于高分辨率图像上仍然花费巨大。幸运的是，如今(2012)的GPU对2D卷积计算进行了大量优化，已经足够训练大型CNN，而且最近(2012)诸如ImageNet的数据集也包含了足够多的已标记数据可以训练如此规模的模型同时又不会引起严重的过拟合。</p>
<h1 id="2-__u6570_u636E_u96C6"><a href="#2-__u6570_u636E_u96C6" class="headerlink" title="2. 数据集"></a>2. 数据集</h1><p>ImageNet的图像分辨率并不固定，而AlexNet需要一个固定维数的输入。因此，作者将图片降采样到固定分256x256的分辨率上。<br>对于一个给定图像，首先将最短边(保持比例)拉伸到256个像素长度，再从拉伸后图像裁剪出中间的一个256x256像素的区块。然后除了在整个训练集图像上计算出的每个像素均值以外，对图像不再做其他操作。总的来说作者基于中心化的原始RGB像素值训练网络。</p>
<h1 id="3-__u7F51_u7EDC_u7ED3_u6784"><a href="#3-__u7F51_u7EDC_u7ED3_u6784" class="headerlink" title="3. 网络结构"></a>3. 网络结构</h1><p>ALexNet共有8个层：5个卷积层和3个全连接层。<br>以下将按照论文作者对于其重要性的评估先后描述一些AlexNet中新的或原先不常用的特性：</p>
<h2 id="3-1_ReLU_u975E_u7EBF_u6027_u6FC0_u6D3B_u51FD_u6570"><a href="#3-1_ReLU_u975E_u7EBF_u6027_u6FC0_u6D3B_u51FD_u6570" class="headerlink" title="3.1 ReLU非线性激活函数"></a>3.1 ReLU非线性激活函数</h2><p>原先常用的饱和非线性激活函数 f(x) = tanh(x) 或 f(x) = 1/(1 + e^-x)在使用梯度下降法训练时远远慢于使用不饱和非线性激活函数 f(x) = max(0, x) 称之为Rectified Linear Unit（ReLU）.</p>

<p>对一个在CIFAR-10上训练的四层卷积神经网络，ReLU达到25%错误率的时间比tanh快了6倍。</p>
<h2 id="3-2__u591AGPU_u8BAD_u7EC3"><a href="#3-2__u591AGPU_u8BAD_u7EC3" class="headerlink" title="3.2 多GPU训练"></a>3.2 多GPU训练</h2><p>单张GTX 580只有3GB显存，这限制了可在其上训练的网络最大大小。 因此我们将网络分在两张GPU上。</p>
<p>这里还有一个额外的技巧：GPU之间只在特定的层通信。例如第三层卷积核使用第二层的所有输出做输入，而第四层的则只使用同一GPU上第三层的输出做输入。</p>
<h2 id="3-3__u5C40_u90E8_u54CD_u5E94_u5F52_u4E00_u5316_uFF08Local_Response_Normalization_uFF0C_LRN_uFF09"><a href="#3-3__u5C40_u90E8_u54CD_u5E94_u5F52_u4E00_u5316_uFF08Local_Response_Normalization_uFF0C_LRN_uFF09" class="headerlink" title="3.3 局部响应归一化（Local Response Normalization， LRN）"></a>3.3 局部响应归一化（Local Response Normalization， LRN）</h2><p>先看方程式：<br><img src="/2017/04/17/paper-notes-alexnet/lrn-func.jpg" alt="lrn-func.jpg" title=""></p>
<p>这个方程式对空间位置相同的n个相邻卷积核(已经过ReLU变换的)输出做Normalization操作，N则是这一层卷积核数量。<br>常量k, n, α, β是超参数。这种操作启发于真实神经元的侧抑制机制(lateral inhibition)。</p>
<p>更为具体的操作过程如下图所示：<br><img src="/2017/04/17/paper-notes-alexnet/lrn.jpg" alt="lrn.jpg" title=""></p>
<p>不过根据<a href="http://cs231n.github.io/convolutional-networks/#normalization-layer" target="_blank" rel="external">CS231n</a>的描述：这些归一化层已经不太常用，应为在实践中它们的作用较小。</p>
<h2 id="3-4__u91CD_u53E0_u6C60_u5316_uFF08Overlapping_Pooling_uFF09"><a href="#3-4__u91CD_u53E0_u6C60_u5316_uFF08Overlapping_Pooling_uFF09" class="headerlink" title="3.4 重叠池化（Overlapping Pooling）"></a>3.4 重叠池化（Overlapping Pooling）</h2><p>传统的Pooling层是不重叠的，而本论文提出使Pooling层重叠可以降低错误率（此处理使Top 1/5错误率下降了0.4/0.3%），而且对防止过拟合有一定(slightly)的效果。</p>
<h2 id="3-5__u6574_u4F53_u7ED3_u6784"><a href="#3-5__u6574_u4F53_u7ED3_u6784" class="headerlink" title="3.5 整体结构"></a>3.5 整体结构</h2><img src="/2017/04/17/paper-notes-alexnet/alexnet-arch.jpg" alt="alexnet-arch.jpg" title="">
<p>其中第一层输入应该为作者笔误，实际应该为227x227x3的输入大小，有96个11x11x3步长为4的卷积核（每个GPU有48个），第二层接受（响应归一化和池化后的）底层的输出，第3、4、5层间则没有Normalization和Pooling层。最后每个全连接层有4096个神经元。</p>
<h1 id="4__u964D_u4F4E_u8FC7_u62DF_u5408"><a href="#4__u964D_u4F4E_u8FC7_u62DF_u5408" class="headerlink" title="4 降低过拟合"></a>4 降低过拟合</h1><p>AlexNet模型共有6000万个参数，现有数据不足以学习这么多参数而不引起过拟合，作者主要采用两种方法来对抗过拟合：</p>
<h2 id="4-1__u6570_u636E_u6269_u589E_uFF08Data_Augmentation_uFF09"><a href="#4-1__u6570_u636E_u6269_u589E_uFF08Data_Augmentation_uFF09" class="headerlink" title="4.1 数据扩增（Data Augmentation）"></a>4.1 数据扩增（Data Augmentation）</h2><p>最简单和常见的减少过拟合的方法就是对图像数据做标签不变变换(label-preserving transformations)来人为增大数据集。<br>在作者实现中，在CPU上使用Python代码生成变换后的图像（而不存储在硬盘上）同时GPU根据前一个batch的数据在做训练。</p>
<p>第一种方法是图像裁剪和水平翻转。从256x256中裁出随机位置的224x224像素（应为227x227）并加上水平翻转。在测试时使用4角+中心+翻转的10张图预测值取平均。</p>
<p>第二种方法是加调整图像RGB通道强度。对整个训练集求来RGB值的PCA，然后不降维但对特征向量乘上一个均值0标准差0.1的高斯随机变量。此处理使top 1错误率下降了1%。</p>
<h2 id="4-2_Dropout"><a href="#4-2_Dropout" class="headerlink" title="4.2 Dropout"></a>4.2 Dropout</h2><p>整合多个模型的预测结果对于减少错误率有显著效果，但这样对于花费数天来训练的神经网络来说花费巨大。但是一种更加高效的方法就是使用dropout。</p>
<p>把神经元的输出以0.5的概率置零，相当于扔掉了(dropped out)，被扔掉的神经元在前馈和反馈中都不起作用。所以每当有一个新输入时，由于Dropout的随机性，相当于在训练一个新的网络，但是这些网络都共享参数。<br>这种策略减少了神经元之间的相互影响，强制网络学习出更鲁棒的features能整合多个随机神经元的输出而不依赖某一个特别的神经元。<br>测试的时候则不dropout了，但每个神经元输出值乘以0.5，作为引入了dropout后预测分布的均值的估计。</p>
<p>作者在前两个全连接层使用了dropout，同时dropout几乎倍增了网络收敛时间。</p>
<h1 id="5__u5177_u4F53_u8BAD_u7EC3_u8FC7_u7A0B"><a href="#5__u5177_u4F53_u8BAD_u7EC3_u8FC7_u7A0B" class="headerlink" title="5 具体训练过程"></a>5 具体训练过程</h1><p>使用Mini-batch stochastic gradient descent，batch_size=128, momentum=0.9, weight_decay=0.0005。</p>
<p>权重更新规则如下：<br><img src="/2017/04/17/paper-notes-alexnet/weight.jpg" alt="weight.jpg" title=""></p>
<p>其中i是迭代次数，v是动量变量(momentum variable), ε是学习率，学习率后面的则是整个batch对于w的平均梯度。</p>
<p>作者使用均值0标准差为0.01的高斯分布初始化权重，对于2、4、5和全连接层的bias初始值1（给ReLU单元提供正的输入加速早期学习），其他层bias初始值为0。</p>
<p>同时其在每一层使用相同学习率并在训练过程中手动调整：每当验证集错误率不再下降时就对学习率除以10.学习率初始值为0.1，并且最后训练完成时下降了3次。整个网络大概在120万张图片训练集上跑了90遍，使用两块GTX580 3G花了5到6天。</p>
<h1 id="6__u7ED3_u8BBA"><a href="#6__u7ED3_u8BBA" class="headerlink" title="6 结论"></a>6 结论</h1><ol>
<li><p>对最终正确率最关键的因素：网络深度（减一层2%、加一层1.7%）&gt; 单层容量（双GPU约1.7%）&gt; LRN（1.4%）&gt; PCA干扰（1%）&gt; 重叠Polling（0.4%）。即，虽然其他的Trick也有明显效果，但决定性的，还是尽可能保持一个足够深、足够大规模的网络。</p>
</li>
<li><p>文中除了常规的case展示，还把倒数第二层的欧氏距离用于衡量图片是否相似，展示了很多图片查询的例子。</p>
</li>
</ol>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[cmder中文显示相关问题解决方案(1.3以上版本)]]></title>
      <url>http://wentaoma.com/2016/08/31/cmder-chinese-encode/</url>
      <content type="html"><![CDATA[<p>cmder虽然Windows命令行的进阶版，虽然好看易用，但其中文编码一直是个问题。网上有不少博客给出解决方案，大部分都已因为版本更新失效。<br>本文解决方案针对1.3以上版本的cmder用户<br><a id="more"></a></p>
<h2 id="u4E2D_u6587_u5B57_u4F53_u91CD_u53E0_u95EE_u9898"><a href="#u4E2D_u6587_u5B57_u4F53_u91CD_u53E0_u95EE_u9898" class="headerlink" title="中文字体重叠问题"></a>中文字体重叠问题</h2><h3 id="u9519_u8BEF_u65B9_u6848_uFF1A"><a href="#u9519_u8BEF_u65B9_u6848_uFF1A" class="headerlink" title="错误方案："></a>错误方案：</h3><blockquote>
<p>需要取消勾选设置中的<code>Monospace</code>选项</p>
</blockquote>
<h3 id="u6B63_u786E_u65B9_u6848_3A"><a href="#u6B63_u786E_u65B9_u6848_3A" class="headerlink" title="正确方案:"></a>正确方案:</h3><p>这个问题在<code>cmder v1.3.0</code>以上版本中已经修复，不需要进行任何操作</p>
<h2 id="ls_u547D_u4EE4_u4E2D_u6587_u8DEF_u5F84/_u6587_u4EF6_u540D_u4E71_u7801"><a href="#ls_u547D_u4EE4_u4E2D_u6587_u8DEF_u5F84/_u6587_u4EF6_u540D_u4E71_u7801" class="headerlink" title="ls命令中文路径/文件名乱码"></a><code>ls</code>命令中文路径/文件名乱码</h2><h3 id="u9519_u8BEF_u65B9_u6848_uFF1A-1"><a href="#u9519_u8BEF_u65B9_u6848_uFF1A-1" class="headerlink" title="错误方案："></a>错误方案：</h3><blockquote>
<p>添加4行命令到cmder/config/aliases文件末尾…</p>
</blockquote>
<p>在<code>cmder v1.3.0</code>以上版本初始创建的<code>cmder/config/user-aliases.cmd</code>文件中已经包含：</p>
<figure class="highlight bat"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls=ls --show-control-chars -F --<span class="built_in">color</span> $*</span><br></pre></td></tr></table></figure>
<p>不需要添加其它命令（实际上添加了也没有效果）</p>
<h3 id="u6B63_u786E_u65B9_u6848_3A-1"><a href="#u6B63_u786E_u65B9_u6848_3A-1" class="headerlink" title="正确方案:"></a>正确方案:</h3><ol>
<li><p><code>win+ctrl+p</code>打开Settings</p>
</li>
<li><p>在Settings &gt; Startup &gt; Environment里添加：<code>set LANG=zh_CN.UTF8</code></p>
</li>
</ol>
<p><strong>PS：</strong></p>
<p>这样修改过<code>ls</code>可以正确显示中文，但<code>ls |more</code>还是会出现乱码。<br>如果改为<code>set LANG=zh_CN.GBK</code>可以解决这个问题。</p>
<p><strong>PPS：</strong></p>
<p>因为<code>cat</code>命令读取文件的编码与此有关，如果改为GBK则<code>cat</code>一个UTF8文件会显示乱码，<br>如果改为UTF8则<code>cat</code>一个GBK文件会显示乱码，<br>具体设置还是看你环境中常用编码。</p>
<h2 id="cd_u8FDB_u4E00_u4E2A_u4E2D_u6587_u76EE_u5F55_uFF0C_u4E2D_u6587_u8DEF_u5F84_u540D_u663E_u793A_u4E71_u7801"><a href="#cd_u8FDB_u4E00_u4E2A_u4E2D_u6587_u76EE_u5F55_uFF0C_u4E2D_u6587_u8DEF_u5F84_u540D_u663E_u793A_u4E71_u7801" class="headerlink" title="cd进一个中文目录，中文路径名显示乱码"></a><code>cd</code>进一个中文目录，中文路径名显示乱码</h2><p>经过上述设置，cmder一般情况下都能正常显示中文</p>
<p>但是如果你进入一个中文路径的话，cmder的路径的中文仍然是乱码<br>好在已经有人解决了这个问题</p>
<p><a href="https://github.com/cmderdev/cmder/pull/1070" target="_blank" rel="external">Parse the original prompt for cwd and env names by janschulz · Pull Request #1070 · cmderdev/cmder · GitHub</a></p>
<h3 id="u89E3_u51B3_u65B9_u6848"><a href="#u89E3_u51B3_u65B9_u6848" class="headerlink" title="解决方案"></a>解决方案</h3><p>只需下载最新的Release（目前是1.3.1）：<br><a href="https://github.com/cmderdev/cmder/releases" target="_blank" rel="external">Releases · cmderdev/cmder · GitHub</a></p>
<p>然后解压覆盖就好</p>
<p>最后附上一张配置完成的示意图:</p>
<img src="/2016/08/31/cmder-chinese-encode/cmder-cn-encode.png" alt="cmder-cn-encode.png" title="">
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[CSS选择器权重及样式优先级]]></title>
      <url>http://wentaoma.com/2016/08/19/CSS-selector-weight/</url>
      <content type="html"><![CDATA[<p>CSS样式应用到元素上的规则可以简述为：先将所有样式按来源、权重排序；然后取权重高的样式，权重相同的样式根据“就近原则”应用。<br>我们先从选择器权重说起：<br><a id="more"></a></p>
<h2 id="CSS_u9009_u62E9_u5668_u6743_u91CD"><a href="#CSS_u9009_u62E9_u5668_u6743_u91CD" class="headerlink" title="CSS选择器权重"></a>CSS选择器权重</h2><p>从高到低可以将CSS选择器权重排列如下：</p>
<p><strong>1. 内联样式<code>inline-style</code>权重最高，权值可以记为1000</strong></p>
<p><strong>2. ID选择器，权值记为100</strong></p>
<p><strong>3. Class选择器、属性选择器、伪类选择器，权值记为10</strong></p>
<p><strong>4. 元素、伪元素选择器，权值记为1</strong></p>
<p><strong>5. 通配符选择器<code>*</code>，权值为0(为什么记为0而不是无权重见下方)，结合符(空格、+，&gt;)不计入权重</strong></p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">h1</span> <span class="rules">&#123;<span class="rule"><span class="attribute">color</span>:<span class="value"> red</span></span>;&#125;</span> <span class="comment">/*权重=0001*/</span></span><br><span class="line"><span class="tag">p</span><span class="class">.siderbar</span> &gt; <span class="tag">em</span> <span class="rules">&#123;<span class="rule"><span class="attribute">color</span>:<span class="value"> purple</span></span>;&#125;</span> <span class="comment">/*权重=0012*/</span></span><br><span class="line"><span class="class">.clearfloat</span><span class="pseudo">:after</span> <span class="rules">&#123;<span class="rule"><span class="attribute">content</span>:<span class="value"><span class="string">""</span></span></span>;<span class="rule"><span class="attribute">display</span>:<span class="value">block</span></span>;<span class="rule"><span class="attribute">clear</span>:<span class="value">both</span></span>;&#125;</span> <span class="comment">/*权重=0020*/</span></span><br><span class="line"><span class="tag">li</span><span class="id">#answer</span> <span class="rules">&#123;<span class="rule"><span class="attribute">color</span>:<span class="value">navy</span></span>;&#125;</span> <span class="comment">/*权重=0101*/</span></span><br></pre></td></tr></table></figure>
<h3 id="u6CE8_u610F_u4E8B_u9879"><a href="#u6CE8_u610F_u4E8B_u9879" class="headerlink" title="注意事项"></a><em>注意事项</em></h3><p><strong>1. 使用属性选择器选择ID，其权重仍为10，如：</strong></p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">ul</span> &gt; <span class="tag">li</span><span class="attr_selector">[id="answer"]</span> <span class="rules">&#123;<span class="rule"><span class="attribute">color</span>:<span class="value">navy</span></span>;&#125;</span> <span class="comment">/*权重=0012*/</span></span><br><span class="line"><span class="tag">li</span><span class="id">#answer</span> <span class="rules">&#123;<span class="rule"><span class="attribute">color</span>:<span class="value">navy</span></span>;&#125;</span> <span class="comment">/*权重=0101*/</span></span><br></pre></td></tr></table></figure>
<p><strong>2. 权重值不会进位，即使有10个元素选择器，其权重也不如一个Class选择器权重高，即<code>000(10)&lt;0010</code></strong></p>
<p><strong>3. 拥有<code>!important</code>声明的样式声明其权重单独计算</strong></p>
<p>如果一个重要声明(有<code>!important</code>)权重和非重要声明(无<code>!important</code>)冲突，重要声明优先，如：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">h1</span> <span class="rules">&#123;<span class="rule"><span class="attribute">color</span>:<span class="value">red <span class="important">!important</span></span></span>;&#125;</span></span><br><span class="line"><span class="id">#title</span> <span class="rules">&#123;<span class="rule"><span class="attribute">color</span>:<span class="value">yellow <span class="important">!important</span></span></span>;&#125;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">h1</span> <span class="attribute">id</span>=<span class="value">"title"</span> <span class="attribute">style</span>=<span class="value">"color:black"</span>&gt;</span>Final Color is YELLOW<span class="tag">&lt;/<span class="title">h1</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>两个重要声明，第二个<code>#title</code>权重更高，且都优先于内联非重要声明的样式。</p>
<p><strong>4. (默认)继承的样式没有权重，会被通配符样式(0权重)覆盖，如：</strong></p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">* <span class="rules">&#123;<span class="rule"><span class="attribute">color</span>:<span class="value">yellow</span></span>;&#125;</span></span><br><span class="line"><span class="tag">h1</span> <span class="rules">&#123;<span class="rule"><span class="attribute">color</span>:<span class="value">red</span></span>;&#125;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">h1</span>&gt;</span>RED and <span class="tag">&lt;<span class="title">em</span>&gt;</span>YELLOW<span class="tag">&lt;/<span class="title">em</span>&gt;</span><span class="tag">&lt;/<span class="title">h1</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><code>em</code>元素继承了<code>h1</code>的color样式，但是因为其无权重，所以会被<code>* {color:yellow;}</code>覆盖。<br>所以通配符选择器往往有一种短路继承的效果。</p>
<p>但是如果在样式中明确指定使用继承属性值，就要看选择器的权重来决定了。比如在上述CSS中加入<code>em{color:inherit;}</code><br><code>em</code>标签文本就显示为继承下来的红色。</p>
<iframe width="100%" height="300" src="http://jsfiddle.net/vcdvuv5p/embedded/css,html,result/light" frameborder="0" allowfullscreen></iframe>
<h2 id="CSS_u6837_u5F0F_u5E94_u7528_u4F18_u5148_u7EA7_u5206_u6790"><a href="#CSS_u6837_u5F0F_u5E94_u7528_u4F18_u5148_u7EA7_u5206_u6790" class="headerlink" title="CSS样式应用优先级分析"></a>CSS样式应用优先级分析</h2><p>了解了选择器权重之后，我们就可以来看一下CSS到底是怎么选择某个元素样式的：</p>
<ol>
<li><p><strong>列出每一条样式规则，每条规则都含有一个匹配给定元素的选择器</strong></p>
</li>
<li><p><strong>按照来源和重要声明对所有样式排序</strong></p>
<p> CSS的来源有三种：</p>
<ul>
<li><p>User Agent Stylesheet 用户代理的默认CSS（浏览器默认的CSS）</p>
</li>
<li><p>Author Stylesheet 开发人员定义的CSS</p>
</li>
<li><p>User Stylesheet 用户自定义的CSS</p>
<p>再考虑到重要声明，可以将样式如下排序（优先级从高到低）：</p>
</li>
</ul>
<ol>
<li><p>User Stylesheet (!important)</p>
</li>
<li><p>Author Stylesheet (!important)</p>
</li>
<li><p>Author Stylesheet (Normal)</p>
</li>
<li><p>User Stylesheet (Normal)</p>
</li>
<li><p>User Agent Stylesheet</p>
</li>
</ol>
</li>
<li><p><strong>按照选择器权重给所有样式声明排序</strong></p>
</li>
<li><p><strong>按照样式声明出现的顺序给其排序</strong></p>
<p> 在相同权重下，样式声明离被设置元素越近优先级别越高，即“就近原则”。</p>
<p> 由于这个原因，才有了推荐的链接样式排序(link-visited-hover-active, LVHA)</p>
 <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="pseudo">:link</span> <span class="rules">&#123;<span class="rule"><span class="attribute">color</span>:<span class="value">blue</span></span>;&#125;</span></span><br><span class="line"><span class="pseudo">:visited</span> <span class="rules">&#123;<span class="rule"><span class="attribute">color</span>:<span class="value">purple</span></span>;&#125;</span></span><br><span class="line"><span class="pseudo">:hover</span> <span class="rules">&#123;<span class="rule"><span class="attribute">color</span>:<span class="value">red</span></span>;&#125;</span></span><br><span class="line"><span class="pseudo">:active</span> <span class="rules">&#123;<span class="rule"><span class="attribute">color</span>:<span class="value">orange</span></span>;&#125;</span></span><br></pre></td></tr></table></figure>
<p> 上述顺序如果换成：</p>
 <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="pseudo">:active</span> <span class="rules">&#123;<span class="rule"><span class="attribute">color</span>:<span class="value">orange</span></span>;&#125;</span></span><br><span class="line"><span class="pseudo">:hover</span> <span class="rules">&#123;<span class="rule"><span class="attribute">color</span>:<span class="value">red</span></span>;&#125;</span></span><br><span class="line"><span class="pseudo">:link</span> <span class="rules">&#123;<span class="rule"><span class="attribute">color</span>:<span class="value">blue</span></span>;&#125;</span></span><br><span class="line"><span class="pseudo">:visited</span> <span class="rules">&#123;<span class="rule"><span class="attribute">color</span>:<span class="value">purple</span></span>;&#125;</span></span><br></pre></td></tr></table></figure>
<p> 则任何链接都不会显示<code>:hover</code>或<code>:active</code>样式。因为任何链接要么已访问，要么未访问，所以前两个样式会被覆盖。</p>
</li>
</ol>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Caffe-Python接口常用API参考]]></title>
      <url>http://wentaoma.com/2016/08/10/caffe-python-common-api-reference/</url>
      <content type="html"><![CDATA[<p>本文整理了pycaffe中常用的API<br><a id="more"></a></p>
<h2 id="Packages_u5BFC_u5165"><a href="#Packages_u5BFC_u5165" class="headerlink" title="Packages导入"></a>Packages导入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> caffe</span><br><span class="line"><span class="keyword">from</span> caffe <span class="keyword">import</span> layers <span class="keyword">as</span> L</span><br><span class="line"><span class="keyword">from</span> caffe <span class="keyword">import</span> params <span class="keyword">as</span> P</span><br></pre></td></tr></table></figure>
<h2 id="Layers_u5B9A_u4E49"><a href="#Layers_u5B9A_u4E49" class="headerlink" title="Layers定义"></a>Layers定义</h2><h3 id="Data_u5C42_u5B9A_u4E49"><a href="#Data_u5C42_u5B9A_u4E49" class="headerlink" title="Data层定义"></a>Data层定义</h3><h4 id="lmdb/leveldb_Data_u5C42_u5B9A_u4E49"><a href="#lmdb/leveldb_Data_u5C42_u5B9A_u4E49" class="headerlink" title="lmdb/leveldb Data层定义"></a>lmdb/leveldb Data层定义</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">L.Data( </span><br><span class="line">        source=lmdb,</span><br><span class="line">        backend=P.Data.LMDB,</span><br><span class="line">        batch_size=batch_size, ntop=<span class="number">2</span>,</span><br><span class="line">        transform_param=dict(</span><br><span class="line">                              crop_size=<span class="number">227</span>,</span><br><span class="line">                              mean_value=[<span class="number">104</span>, <span class="number">117</span>, <span class="number">123</span>],</span><br><span class="line">                              mirror=<span class="keyword">True</span></span><br><span class="line">                              )</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<h4 id="HDF5_Data_u5C42_u5B9A_u4E49"><a href="#HDF5_Data_u5C42_u5B9A_u4E49" class="headerlink" title="HDF5 Data层定义"></a>HDF5 Data层定义</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">L.HDF5Data(</span><br><span class="line">            hdf5_data_param=&#123;</span><br><span class="line">                            <span class="string">'source'</span>: <span class="string">'./training_data_paths.txt'</span>,  </span><br><span class="line">                            <span class="string">'batch_size'</span>: <span class="number">64</span></span><br><span class="line">                            &#125;,</span><br><span class="line">            include=&#123;</span><br><span class="line">                    <span class="string">'phase'</span>: caffe.TRAIN</span><br><span class="line">                    &#125;</span><br><span class="line">            )</span><br></pre></td></tr></table></figure>
<h4 id="ImageData_Data_u5C42_u5B9A_u4E49"><a href="#ImageData_Data_u5C42_u5B9A_u4E49" class="headerlink" title="ImageData Data层定义"></a>ImageData Data层定义</h4><p>适用于txt文件一行记录一张图片的数据源</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">L.ImageData(</span><br><span class="line">                source=list_path,</span><br><span class="line">                batch_size=batch_size,</span><br><span class="line">                new_width=<span class="number">48</span>,</span><br><span class="line">                new_height=<span class="number">48</span>,</span><br><span class="line">                ntop=<span class="number">2</span>,</span><br><span class="line">                ransform_param=dict(crop_size=<span class="number">40</span>,mirror=<span class="keyword">True</span>)</span><br><span class="line">                )</span><br></pre></td></tr></table></figure>
<h3 id="Convloution_u5C42_u5B9A_u4E49"><a href="#Convloution_u5C42_u5B9A_u4E49" class="headerlink" title="Convloution层定义"></a>Convloution层定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">L.Convolution(  </span><br><span class="line">                bottom, </span><br><span class="line">                kernel_size=ks, </span><br><span class="line">                stride=stride,</span><br><span class="line">                num_output=nout, </span><br><span class="line">                pad=pad, </span><br><span class="line">                group=group</span><br><span class="line">                )</span><br></pre></td></tr></table></figure>
<h3 id="LRN_u5C42_u5B9A_u4E49"><a href="#LRN_u5C42_u5B9A_u4E49" class="headerlink" title="LRN层定义"></a>LRN层定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">L.LRN(</span><br><span class="line">        bottom, </span><br><span class="line">        local_size=<span class="number">5</span>, </span><br><span class="line">        alpha=<span class="number">1e-4</span>, </span><br><span class="line">        beta=<span class="number">0.75</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<h3 id="Activation_u5C42_u5B9A_u4E49"><a href="#Activation_u5C42_u5B9A_u4E49" class="headerlink" title="Activation层定义"></a>Activation层定义</h3><h4 id="ReLU_u5C42_u5B9A_u4E49"><a href="#ReLU_u5C42_u5B9A_u4E49" class="headerlink" title="ReLU层定义"></a>ReLU层定义</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">L.ReLU(</span><br><span class="line">        bottom, </span><br><span class="line">        in_place=<span class="keyword">True</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<h3 id="Pooling_u5C42_u5B9A_u4E49"><a href="#Pooling_u5C42_u5B9A_u4E49" class="headerlink" title="Pooling层定义"></a>Pooling层定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">L.Pooling(</span><br><span class="line">            bottom,</span><br><span class="line">            pool=P.Pooling.MAX, </span><br><span class="line">            kernel_size=ks, </span><br><span class="line">            stride=stride</span><br><span class="line">            )</span><br></pre></td></tr></table></figure>
<h3 id="FullConnect_u5C42_u5B9A_u4E49"><a href="#FullConnect_u5C42_u5B9A_u4E49" class="headerlink" title="FullConnect层定义"></a>FullConnect层定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">L.InnerProduct(</span><br><span class="line">                bottom, </span><br><span class="line">                num_output=nout</span><br><span class="line">                )</span><br></pre></td></tr></table></figure>
<h3 id="Dropout_u5C42_u5B9A_u4E49"><a href="#Dropout_u5C42_u5B9A_u4E49" class="headerlink" title="Dropout层定义"></a>Dropout层定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">L.Dropout(</span><br><span class="line">            bottom, </span><br><span class="line">            in_place=<span class="keyword">True</span></span><br><span class="line">            )</span><br></pre></td></tr></table></figure>
<h3 id="Loss_u5C42_u5B9A_u4E49"><a href="#Loss_u5C42_u5B9A_u4E49" class="headerlink" title="Loss层定义"></a>Loss层定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">L.SoftmaxWithLoss(</span><br><span class="line">                    bottom, </span><br><span class="line">                    label</span><br><span class="line">                    )</span><br></pre></td></tr></table></figure>
<h3 id="Accuracy_u5C42_u5B9A_u4E49"><a href="#Accuracy_u5C42_u5B9A_u4E49" class="headerlink" title="Accuracy层定义"></a>Accuracy层定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">L.Accuracy(</span><br><span class="line">            bottom,</span><br><span class="line">            label</span><br><span class="line">            )</span><br></pre></td></tr></table></figure>
<h3 id="u8F6C_u6362_u4E3Aproto_u6587_u672C"><a href="#u8F6C_u6362_u4E3Aproto_u6587_u672C" class="headerlink" title="转换为proto文本"></a>转换为proto文本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">caffe.to_proto(</span><br><span class="line">                loss, </span><br><span class="line">                acc     <span class="comment">#训练阶段可以删去Accuracy层</span></span><br><span class="line">                )</span><br></pre></td></tr></table></figure>
<h2 id="Solver_u5B9A_u4E49"><a href="#Solver_u5B9A_u4E49" class="headerlink" title="Solver定义"></a>Solver定义</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> caffe.proto <span class="keyword">import</span> caffe_pb2</span><br><span class="line"></span><br><span class="line">s = caffe_pb2.SolverParameter()</span><br><span class="line"></span><br><span class="line">path=<span class="string">'/home/xxx/data/'</span></span><br><span class="line">solver_file=path+<span class="string">'solver.prototxt'</span>     <span class="comment">#solver文件保存位置</span></span><br><span class="line"></span><br><span class="line">s.train_net = path+<span class="string">'train.prototxt'</span>     <span class="comment"># 训练配置文件</span></span><br><span class="line">s.test_net.append(path+<span class="string">'val.prototxt'</span>)  <span class="comment"># 测试配置文件</span></span><br><span class="line">s.test_interval = <span class="number">782</span>                   <span class="comment"># 测试间隔</span></span><br><span class="line">s.test_iter.append(<span class="number">313</span>)                 <span class="comment"># 测试迭代次数</span></span><br><span class="line">s.max_iter = <span class="number">78200</span>                      <span class="comment"># 最大迭代次数</span></span><br><span class="line"></span><br><span class="line">s.base_lr = <span class="number">0.001</span>                       <span class="comment"># 基础学习率</span></span><br><span class="line">s.momentum = <span class="number">0.9</span>                        <span class="comment"># momentum系数</span></span><br><span class="line">s.weight_decay = <span class="number">5e-4</span>                   <span class="comment"># 权值衰减系数</span></span><br><span class="line">s.lr_policy = <span class="string">'step'</span>                    <span class="comment"># 学习率衰减方法</span></span><br><span class="line">s.stepsize=<span class="number">26067</span>                        <span class="comment"># 此值仅对step方法有效</span></span><br><span class="line">s.gamma = <span class="number">0.1</span>                           <span class="comment"># 学习率衰减指数</span></span><br><span class="line">s.display = <span class="number">782</span>                         <span class="comment"># 屏幕日志显示间隔</span></span><br><span class="line">s.snapshot = <span class="number">7820</span></span><br><span class="line">s.snapshot_prefix = <span class="string">'shapshot'</span></span><br><span class="line">s.type = “SGD”                          <span class="comment"># 优化算法</span></span><br><span class="line">s.solver_mode = caffe_pb2.SolverParameter.GPU</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(solver_file, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(str(s))</span><br></pre></td></tr></table></figure>
<h2 id="Model_u8BAD_u7EC3"><a href="#Model_u8BAD_u7EC3" class="headerlink" title="Model训练"></a>Model训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练设置</span></span><br><span class="line"><span class="comment"># 使用GPU</span></span><br><span class="line">caffe.set_device(gpu_id) <span class="comment"># 若不设置,默认为0</span></span><br><span class="line">caffe.set_mode_gpu()</span><br><span class="line"><span class="comment"># 使用CPU</span></span><br><span class="line">caffe.set_mode_cpu()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载Solver，有两种常用方法</span></span><br><span class="line"><span class="comment"># 1. 无论模型中Slover类型是什么统一设置为SGD</span></span><br><span class="line">solver = caffe.SGDSolver(<span class="string">'/home/xxx/data/solver.prototxt'</span>) </span><br><span class="line"><span class="comment"># 2. 根据solver的prototxt中solver_type读取，默认为SGD</span></span><br><span class="line">solver = caffe.get_solver(<span class="string">'/home/xxx/data/solver.prototxt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="comment"># 1.1 前向传播</span></span><br><span class="line">solver.net.forward()  <span class="comment"># train net</span></span><br><span class="line">solver.test_nets[<span class="number">0</span>].forward()  <span class="comment"># test net (there can be more than one)</span></span><br><span class="line"><span class="comment"># 1.2 反向传播,计算梯度</span></span><br><span class="line">solver.net.backward()</span><br><span class="line"><span class="comment"># 2. 进行一次前向传播一次反向传播并根据梯度更新参数</span></span><br><span class="line">solver.step(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 3. 根据solver文件中设置进行完整model训练</span></span><br><span class="line">solver.solve()</span><br></pre></td></tr></table></figure>
<p>如果想在训练过程中保存模型参数，调用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">solver.net.save(<span class="string">'mymodel.caffemodel'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="u5206_u7C7B_u56FE_u7247"><a href="#u5206_u7C7B_u56FE_u7247" class="headerlink" title="分类图片"></a>分类图片</h2><h3 id="u52A0_u8F7DModel_u6570_u636E"><a href="#u52A0_u8F7DModel_u6570_u636E" class="headerlink" title="加载Model数据"></a>加载Model数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net = caffe.Net(</span><br><span class="line">        deploy_prototxt_path,   <span class="comment"># 用于分类的网络定义文件路径</span></span><br><span class="line">        caffe_model_path,       <span class="comment"># 训练好模型路径</span></span><br><span class="line">        caffe.TEST              <span class="comment"># 设置为测试阶段</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<h3 id="u4E2D_u503C_u6587_u4EF6_u8F6C_u6362"><a href="#u4E2D_u503C_u6587_u4EF6_u8F6C_u6362" class="headerlink" title="中值文件转换"></a>中值文件转换</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#　编写一个函数，将二进制的均值转换为python的均值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_mean</span><span class="params">(binMean,npyMean)</span>:</span></span><br><span class="line">    blob = caffe.proto.caffe_pb2.BlobProto()</span><br><span class="line">    bin_mean = open(binMean, <span class="string">'rb'</span> ).read()</span><br><span class="line">    blob.ParseFromString(bin_mean)</span><br><span class="line">    arr = np.array( caffe.io.blobproto_to_array(blob) )</span><br><span class="line">    npy_mean = arr[<span class="number">0</span>]</span><br><span class="line">    np.save(npyMean, npy_mean )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用函数转换均值</span></span><br><span class="line">binMean=<span class="string">'examples/cifar10/mean.binaryproto'</span></span><br><span class="line">npyMean=<span class="string">'examples/cifar10/mean.npy'</span></span><br><span class="line">convert_mean(binMean,npyMean)</span><br></pre></td></tr></table></figure>
<h3 id="u56FE_u7247_u9884_u5904_u7406"><a href="#u56FE_u7247_u9884_u5904_u7406" class="headerlink" title="图片预处理"></a>图片预处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设定图片的shape格式为网络data层格式</span></span><br><span class="line">transformer = caffe.io.Transformer(&#123;<span class="string">'data'</span>: net.blobs[<span class="string">'data'</span>].data.shape&#125;)</span><br><span class="line"><span class="comment"># 改变维度的顺序，由原始图片维度(width, height, channel)变为(channel, width, height)</span></span><br><span class="line">transformer.set_transpose(<span class="string">'data'</span>, (<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>)) </span><br><span class="line"><span class="comment"># 减去均值,注意要先将binaryproto格式均值文件转换为npy格式[此步根据训练model时设置可选]</span></span><br><span class="line">transformer.set_mean(<span class="string">'data'</span>, np.load(mean_file_path).mean(<span class="number">1</span>).mean(<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 缩放到[0，255]之间</span></span><br><span class="line">transformer.set_raw_scale(<span class="string">'data'</span>, <span class="number">255</span>)</span><br><span class="line"><span class="comment"># 交换通道，将图片由RGB变为BGR</span></span><br><span class="line">transformer.set_channel_swap(<span class="string">'data'</span>, (<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载图片</span></span><br><span class="line">im=caffe.io.load_image(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行上面设置的图片预处理操作，并将图片载入到blob中</span></span><br><span class="line">net.blobs[<span class="string">'data'</span>].data[...] = transformer.preprocess(<span class="string">'data'</span>,im)</span><br></pre></td></tr></table></figure>
<h3 id="u6267_u884C_u6D4B_u8BD5"><a href="#u6267_u884C_u6D4B_u8BD5" class="headerlink" title="执行测试"></a>执行测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#执行测试</span></span><br><span class="line">out = net.forward()</span><br><span class="line"></span><br><span class="line">labels = np.loadtxt(labels_filename, str, delimiter=<span class="string">'\t'</span>)   <span class="comment">#读取类别名称文件</span></span><br><span class="line">prob= net.blobs[<span class="string">'Softmax1'</span>].data[<span class="number">0</span>].flatten() <span class="comment">#取出最后一层（Softmax）属于某个类别的概率值，并打印</span></span><br><span class="line"><span class="keyword">print</span> prob</span><br><span class="line">order=prob.argsort()[<span class="number">0</span>]  <span class="comment">#将概率值排序，取出最大值所在的序号 </span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'the class is:'</span>,labels[order]   <span class="comment">#将该序号转换成对应的类别名称，并打印</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取出前五个较大值所在的序号</span></span><br><span class="line">top_inds = prob.argsort()[::-<span class="number">1</span>][:<span class="number">5</span>]</span><br><span class="line"><span class="keyword">print</span> <span class="string">'probabilities and labels:'</span> zip(prob[top_inds], labels[top_inds])</span><br></pre></td></tr></table></figure>
<h3 id="u5404_u5C42_u4FE1_u606F_u663E_u793A"><a href="#u5404_u5C42_u4FE1_u606F_u663E_u793A" class="headerlink" title="各层信息显示"></a>各层信息显示</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># params显示：layer名，w，b</span></span><br><span class="line"><span class="keyword">for</span> layer_name, param <span class="keyword">in</span> net.params.items():</span><br><span class="line">    <span class="keyword">print</span> layer_name + <span class="string">'\t'</span> + str(param[<span class="number">0</span>].data.shape), str(param[<span class="number">1</span>].data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># blob显示：layer名，输出的blob维度</span></span><br><span class="line"><span class="keyword">for</span> layer_name, blob <span class="keyword">in</span> net.blobs.items():</span><br><span class="line">    <span class="keyword">print</span> layer_name + <span class="string">'\t'</span> + str(blob.data.shape)</span><br></pre></td></tr></table></figure>
<h3 id="u81EA_u5B9A_u4E49_u51FD_u6570_3A_u53C2_u6570/_u5377_u79EF_u7ED3_u679C_u53EF_u89C6_u5316"><a href="#u81EA_u5B9A_u4E49_u51FD_u6570_3A_u53C2_u6570/_u5377_u79EF_u7ED3_u679C_u53EF_u89C6_u5316" class="headerlink" title="自定义函数:参数/卷积结果可视化"></a>自定义函数:参数/卷积结果可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.image <span class="keyword">as</span> mpimg</span><br><span class="line"><span class="keyword">import</span> caffe</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_data</span><span class="params">(data, padsize=<span class="number">1</span>, padval=<span class="number">0</span>)</span>:</span></span><br><span class="line"><span class="string">"""Take an array of shape (n, height, width) or (n, height, width, 3)</span><br><span class="line">       and visualize each (height, width) thing in a grid of size approx. sqrt(n) by sqrt(n)"""</span></span><br><span class="line">    <span class="comment"># data归一化</span></span><br><span class="line">    data -= data.min()</span><br><span class="line">    data /= data.max()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 根据data中图片数量data.shape[0]，计算最后输出时每行每列图片数n</span></span><br><span class="line">    n = int(np.ceil(np.sqrt(data.shape[<span class="number">0</span>])))</span><br><span class="line">    <span class="comment"># padding = ((图片个数维度的padding),(图片高的padding), (图片宽的padding), ....)</span></span><br><span class="line">    padding = ((<span class="number">0</span>, n ** <span class="number">2</span> - data.shape[<span class="number">0</span>]), (<span class="number">0</span>, padsize), (<span class="number">0</span>, padsize)) + ((<span class="number">0</span>, <span class="number">0</span>),) * (data.ndim - <span class="number">3</span>)</span><br><span class="line">    data = np.pad(data, padding, mode=<span class="string">'constant'</span>, constant_values=(padval, padval))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 先将padding后的data分成n*n张图像</span></span><br><span class="line">    data = data.reshape((n, n) + data.shape[<span class="number">1</span>:]).transpose((<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>) + tuple(range(<span class="number">4</span>, data.ndim + <span class="number">1</span>)))</span><br><span class="line">    <span class="comment"># 再将（n, W, n, H）变换成(n*w, n*H)</span></span><br><span class="line">    data = data.reshape((n * data.shape[<span class="number">1</span>], n * data.shape[<span class="number">3</span>]) + data.shape[<span class="number">4</span>:])</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.imshow(data,cmap=<span class="string">'gray'</span>)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例：显示第一个卷积层的输出数据和权值（filter）</span></span><br><span class="line"><span class="keyword">print</span> net.blobs[<span class="string">'conv1'</span>].data[<span class="number">0</span>].shape</span><br><span class="line">show_data(net.blobs[<span class="string">'conv1'</span>].data[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">print</span> net.params[<span class="string">'conv1'</span>][<span class="number">0</span>].data.shape</span><br><span class="line">show_data(net.params[<span class="string">'conv1'</span>][<span class="number">0</span>].data.reshape(<span class="number">32</span>*<span class="number">3</span>,<span class="number">5</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>
<h3 id="u81EA_u5B9A_u4E49_uFF1A_u8BAD_u7EC3_u8FC7_u7A0BLoss_26amp_3BAccuracy_u53EF_u89C6_u5316"><a href="#u81EA_u5B9A_u4E49_uFF1A_u8BAD_u7EC3_u8FC7_u7A0BLoss_26amp_3BAccuracy_u53EF_u89C6_u5316" class="headerlink" title="自定义：训练过程Loss&amp;Accuracy可视化"></a>自定义：训练过程Loss&amp;Accuracy可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  </span><br><span class="line"><span class="keyword">import</span> caffe   </span><br><span class="line">caffe.set_device(<span class="number">0</span>)  </span><br><span class="line">caffe.set_mode_gpu()   </span><br><span class="line"><span class="comment"># 使用SGDSolver，即随机梯度下降算法  </span></span><br><span class="line">solver = caffe.SGDSolver(<span class="string">'/home/xxx/mnist/solver.prototxt'</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 等价于solver文件中的max_iter，即最大解算次数  </span></span><br><span class="line">niter = <span class="number">10000</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 每隔100次收集一次loss数据  </span></span><br><span class="line">display= <span class="number">100</span>  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 每次测试进行100次解算 </span></span><br><span class="line">test_iter = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 每500次训练进行一次测试</span></span><br><span class="line">test_interval =<span class="number">500</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">#初始化 </span></span><br><span class="line">train_loss = zeros(ceil(niter * <span class="number">1.0</span> / display))   </span><br><span class="line">test_loss = zeros(ceil(niter * <span class="number">1.0</span> / test_interval))  </span><br><span class="line">test_acc = zeros(ceil(niter * <span class="number">1.0</span> / test_interval))  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 辅助变量  </span></span><br><span class="line">_train_loss = <span class="number">0</span>; _test_loss = <span class="number">0</span>; _accuracy = <span class="number">0</span>  </span><br><span class="line"><span class="comment"># 进行解算  </span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(niter):  </span><br><span class="line">    <span class="comment"># 进行一次解算  </span></span><br><span class="line">    solver.step(<span class="number">1</span>)  </span><br><span class="line">    <span class="comment"># 统计train loss  </span></span><br><span class="line">    _train_loss += solver.net.blobs[<span class="string">'SoftmaxWithLoss1'</span>].data  </span><br><span class="line">    <span class="keyword">if</span> it % display == <span class="number">0</span>:  </span><br><span class="line">        <span class="comment"># 计算平均train loss  </span></span><br><span class="line">        train_loss[it // display] = _train_loss / display  </span><br><span class="line">        _train_loss = <span class="number">0</span>  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">if</span> it % test_interval == <span class="number">0</span>:  </span><br><span class="line">        <span class="keyword">for</span> test_it <span class="keyword">in</span> range(test_iter):  </span><br><span class="line">            <span class="comment"># 进行一次测试  </span></span><br><span class="line">            solver.test_nets[<span class="number">0</span>].forward()  </span><br><span class="line">            <span class="comment"># 计算test loss  </span></span><br><span class="line">            _test_loss += solver.test_nets[<span class="number">0</span>].blobs[<span class="string">'SoftmaxWithLoss1'</span>].data  </span><br><span class="line">            <span class="comment"># 计算test accuracy  </span></span><br><span class="line">            _accuracy += solver.test_nets[<span class="number">0</span>].blobs[<span class="string">'Accuracy1'</span>].data  </span><br><span class="line">        <span class="comment"># 计算平均test loss  </span></span><br><span class="line">        test_loss[it / test_interval] = _test_loss / test_iter  </span><br><span class="line">        <span class="comment"># 计算平均test accuracy  </span></span><br><span class="line">        test_acc[it / test_interval] = _accuracy / test_iter  </span><br><span class="line">        _test_loss = <span class="number">0</span>  </span><br><span class="line">        _accuracy = <span class="number">0</span>  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 绘制train loss、test loss和accuracy曲线  </span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'\nplot the train loss and test accuracy\n'</span>  </span><br><span class="line">_, ax1 = plt.subplots()  </span><br><span class="line">ax2 = ax1.twinx()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># train loss -&gt; 绿色  </span></span><br><span class="line">ax1.plot(display * arange(len(train_loss)), train_loss, <span class="string">'g'</span>)  </span><br><span class="line"><span class="comment"># test loss -&gt; 黄色  </span></span><br><span class="line">ax1.plot(test_interval * arange(len(test_loss)), test_loss, <span class="string">'y'</span>)  </span><br><span class="line"><span class="comment"># test accuracy -&gt; 红色  </span></span><br><span class="line">ax2.plot(test_interval * arange(len(test_acc)), test_acc, <span class="string">'r'</span>)  </span><br><span class="line">  </span><br><span class="line">ax1.set_xlabel(<span class="string">'iteration'</span>)  </span><br><span class="line">ax1.set_ylabel(<span class="string">'loss'</span>)  </span><br><span class="line">ax2.set_ylabel(<span class="string">'accuracy'</span>)  </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[使用Caffe的Python接口进行Cifar10可视化]]></title>
      <url>http://wentaoma.com/2016/08/08/caffe-cifar10-model-visualization-with-python/</url>
      <content type="html"><![CDATA[<p>根据训练好的cifar10数据的model，从测试图片中选出一张进行测试，并进行网络模型、卷积结果及参数可视化<br>注意：本文中代码运行在windows+ipython notebook下，已事先配置好caffe的python接口<br><a id="more"></a></p>
<h2 id="u5BFC_u5165_u5FC5_u9700_u7684_u5305"><a href="#u5BFC_u5165_u5FC5_u9700_u7684_u5305" class="headerlink" title="导入必需的包"></a>导入必需的包</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.image <span class="keyword">as</span> mpimg</span><br><span class="line"><span class="keyword">import</span> caffe</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br></pre></td></tr></table></figure>
<h2 id="u8F7D_u5165_u7F51_u7EDC_u6A21_u578B"><a href="#u8F7D_u5165_u7F51_u7EDC_u6A21_u578B" class="headerlink" title="载入网络模型"></a>载入网络模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 载入模型，并显示各层数据信息</span></span><br><span class="line">caffe.set_mode_gpu()</span><br><span class="line">net = caffe.Net(<span class="string">'examples/cifar10/cifar10_quick.prototxt'</span>,</span><br><span class="line">                <span class="string">'examples/cifar10/cifar10_quick_iter_5000.caffemodel.h5'</span>,</span><br><span class="line">                caffe.TEST)</span><br><span class="line">[(k, v.data.shape) <span class="keyword">for</span> k, v <span class="keyword">in</span> net.blobs.items()]</span><br></pre></td></tr></table></figure>
<pre><code>[(&apos;data&apos;, (1L, 3L, 32L, 32L)),
 (&apos;conv1&apos;, (1L, 32L, 32L, 32L)),
 (&apos;pool1&apos;, (1L, 32L, 16L, 16L)),
 (&apos;conv2&apos;, (1L, 32L, 16L, 16L)),
 (&apos;pool2&apos;, (1L, 32L, 8L, 8L)),
 (&apos;conv3&apos;, (1L, 64L, 8L, 8L)),
 (&apos;pool3&apos;, (1L, 64L, 4L, 4L)),
 (&apos;ip1&apos;, (1L, 64L)),
 (&apos;ip2&apos;, (1L, 10L)),
 (&apos;prob&apos;, (1L, 10L))]
</code></pre><h2 id="u53EF_u89C6_u5316_u7F51_u7EDC_u6A21_u578B"><a href="#u53EF_u89C6_u5316_u7F51_u7EDC_u6A21_u578B" class="headerlink" title="可视化网络模型"></a>可视化网络模型</h2><p>使用GraphViz+Caffe的draw_net.py来可视化网络模型</p>
<figure class="highlight bat"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">Rem 运行以下命令前必需先安装配置GraphViz</span></span><br><span class="line"><span class="comment">Rem --rankdir参数为网络方向,BT代表图片上网络从底至顶绘出</span></span><br><span class="line">python ./Build/x64/Release/pycaffe/draw_net.py examples/cifar10/cifar10_quick_train_test.prototxt examples/cifar10/cifar-quick.png --rankdir=BT</span><br></pre></td></tr></table></figure>
<pre><code>Drawing net to examples/cifar10/cifar-quick.png
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#显示模型图片</span></span><br><span class="line">net_im = mpimg.imread(<span class="string">'examples/cifar10/cifar-quick.png'</span>)</span><br><span class="line">plt.imshow(net_im)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(-0.5, 904.5, 2079.5, -0.5)
</code></pre><img src="/2016/08/08/caffe-cifar10-model-visualization-with-python/output_7_1.png" alt="output_7_1.png" title="">
<h2 id="u52A0_u8F7D_u6D4B_u8BD5_u56FE_u7247"><a href="#u52A0_u8F7D_u6D4B_u8BD5_u56FE_u7247" class="headerlink" title="加载测试图片"></a>加载测试图片</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载测试图片，并显示</span></span><br><span class="line">im = caffe.io.load_image(<span class="string">'examples/cifar10/cat.jpg'</span>)</span><br><span class="line"><span class="keyword">print</span> im.shape</span><br><span class="line">plt.imshow(im)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(1200L, 1600L, 3L)

(-0.5, 1599.5, 1199.5, -0.5)
</code></pre><img src="/2016/08/08/caffe-cifar10-model-visualization-with-python/output_8_2.jpg" alt="output_8_2.jpg" title="">
<h2 id="u8F6C_u6362_u5747_u503C"><a href="#u8F6C_u6362_u5747_u503C" class="headerlink" title="转换均值"></a>转换均值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#　编写一个函数，将二进制的均值转换为python的均值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_mean</span><span class="params">(binMean,npyMean)</span>:</span></span><br><span class="line">    blob = caffe.proto.caffe_pb2.BlobProto()</span><br><span class="line">    bin_mean = open(binMean, <span class="string">'rb'</span> ).read()</span><br><span class="line">    blob.ParseFromString(bin_mean)</span><br><span class="line">    arr = np.array( caffe.io.blobproto_to_array(blob) )</span><br><span class="line">    npy_mean = arr[<span class="number">0</span>]</span><br><span class="line">    np.save(npyMean, npy_mean )</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调用函数转换均值</span></span><br><span class="line">binMean=<span class="string">'examples/cifar10/mean.binaryproto'</span></span><br><span class="line">npyMean=<span class="string">'examples/cifar10/mean.npy'</span></span><br><span class="line">convert_mean(binMean,npyMean)</span><br></pre></td></tr></table></figure>
<h2 id="u5C06_u56FE_u7247_u8F7D_u5165Blob"><a href="#u5C06_u56FE_u7247_u8F7D_u5165Blob" class="headerlink" title="将图片载入Blob"></a>将图片载入Blob</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将图片载入blob中,并减去均值</span></span><br><span class="line">transformer = caffe.io.Transformer(&#123;<span class="string">'data'</span>: net.blobs[<span class="string">'data'</span>].data.shape&#125;)</span><br><span class="line">transformer.set_transpose(<span class="string">'data'</span>, (<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line">transformer.set_mean(<span class="string">'data'</span>, np.load(npyMean).mean(<span class="number">1</span>).mean(<span class="number">1</span>)) <span class="comment"># 减去均值</span></span><br><span class="line">transformer.set_raw_scale(<span class="string">'data'</span>, <span class="number">255</span>)  </span><br><span class="line">transformer.set_channel_swap(<span class="string">'data'</span>, (<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>))</span><br><span class="line">net.blobs[<span class="string">'data'</span>].data[...] = transformer.preprocess(<span class="string">'data'</span>,im)</span><br><span class="line">inputData=net.blobs[<span class="string">'data'</span>].data</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#显示减去均值前后的数据</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>),plt.title(<span class="string">"origin"</span>)</span><br><span class="line">plt.imshow(im)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>),plt.title(<span class="string">"subtract mean"</span>)</span><br><span class="line">plt.imshow(transformer.deprocess(<span class="string">'data'</span>, inputData[<span class="number">0</span>]))</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(-0.5, 31.5, 31.5, -0.5)
</code></pre><img src="/2016/08/08/caffe-cifar10-model-visualization-with-python/output_12_1.jpg" alt="output_12_1.jpg" title="">
<h2 id="u7F16_u5199_u7528_u4E8E_u53C2_u6570/_u5377_u79EF_u7ED3_u679C_u53EF_u89C6_u5316_u7684_u51FD_u6570"><a href="#u7F16_u5199_u7528_u4E8E_u53C2_u6570/_u5377_u79EF_u7ED3_u679C_u53EF_u89C6_u5316_u7684_u51FD_u6570" class="headerlink" title="编写用于参数/卷积结果可视化的函数"></a>编写用于参数/卷积结果可视化的函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#　编写一个函数，用于显示各层数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_data</span><span class="params">(data, padsize=<span class="number">1</span>, padval=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="comment"># data归一化</span></span><br><span class="line">    data -= data.min()</span><br><span class="line">    data /= data.max()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 根据data中图片数量data.shape[0]，计算最后输出时每行每列图片数n</span></span><br><span class="line">    n = int(np.ceil(np.sqrt(data.shape[<span class="number">0</span>])))</span><br><span class="line">    <span class="comment"># padding = ((图片个数维度的padding),(图片高的padding), (图片宽的padding), ....)</span></span><br><span class="line">    padding = ((<span class="number">0</span>, n ** <span class="number">2</span> - data.shape[<span class="number">0</span>]), (<span class="number">0</span>, padsize), (<span class="number">0</span>, padsize)) + ((<span class="number">0</span>, <span class="number">0</span>),) * (data.ndim - <span class="number">3</span>)</span><br><span class="line">    data = np.pad(data, padding, mode=<span class="string">'constant'</span>, constant_values=(padval, padval))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 先将padding后的data分成n*n张图像</span></span><br><span class="line">    data = data.reshape((n, n) + data.shape[<span class="number">1</span>:]).transpose((<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>) + tuple(range(<span class="number">4</span>, data.ndim + <span class="number">1</span>)))</span><br><span class="line">    <span class="comment"># 再将（n, W, n, H）变换成(n*w, n*H)</span></span><br><span class="line">    data = data.reshape((n * data.shape[<span class="number">1</span>], n * data.shape[<span class="number">3</span>]) + data.shape[<span class="number">4</span>:])</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.imshow(data,cmap=<span class="string">'gray'</span>)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="u53EF_u89C6_u5316_u5404_u5C42_u6570_u636E"><a href="#u53EF_u89C6_u5316_u5404_u5C42_u6570_u636E" class="headerlink" title="可视化各层数据"></a>可视化各层数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 运行模型并显示第一个卷积层的输出数据和权值（filter）</span></span><br><span class="line">net.forward()</span><br><span class="line"><span class="keyword">print</span> net.blobs[<span class="string">'conv1'</span>].data[<span class="number">0</span>].shape</span><br><span class="line">show_data(net.blobs[<span class="string">'conv1'</span>].data[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">print</span> net.params[<span class="string">'conv1'</span>][<span class="number">0</span>].data.shape</span><br><span class="line">show_data(net.params[<span class="string">'conv1'</span>][<span class="number">0</span>].data.reshape(<span class="number">32</span>*<span class="number">3</span>,<span class="number">5</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>
<pre><code>(32L, 32L, 32L)
(32L, 3L, 5L, 5L)
</code></pre><img src="/2016/08/08/caffe-cifar10-model-visualization-with-python/output_14_1.png" alt="output_14_1.png" title="">
<img src="/2016/08/08/caffe-cifar10-model-visualization-with-python/output_14_2.png" alt="output_14_2.png" title="">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示第一次pooling后的输出数据</span></span><br><span class="line">show_data(net.blobs[<span class="string">'pool1'</span>].data[<span class="number">0</span>])</span><br><span class="line">net.blobs[<span class="string">'pool1'</span>].data.shape</span><br></pre></td></tr></table></figure>
<pre><code>(1L, 32L, 16L, 16L)
</code></pre><img src="/2016/08/08/caffe-cifar10-model-visualization-with-python/output_15_1.png" alt="output_15_1.png" title="">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示第二次卷积后的输出数据以及相应的权值（filter）</span></span><br><span class="line">show_data(net.blobs[<span class="string">'conv2'</span>].data[<span class="number">0</span>],padval=<span class="number">0.5</span>)</span><br><span class="line"><span class="keyword">print</span> net.blobs[<span class="string">'conv2'</span>].data.shape</span><br><span class="line">show_data(net.params[<span class="string">'conv2'</span>][<span class="number">0</span>].data.reshape(<span class="number">32</span>**<span class="number">2</span>,<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line"><span class="keyword">print</span> net.params[<span class="string">'conv2'</span>][<span class="number">0</span>].data.shape</span><br></pre></td></tr></table></figure>
<pre><code>(1L, 32L, 16L, 16L)
(32L, 32L, 5L, 5L)
</code></pre><img src="/2016/08/08/caffe-cifar10-model-visualization-with-python/output_16_1.png" alt="output_16_1.png" title="">
<img src="/2016/08/08/caffe-cifar10-model-visualization-with-python/output_16_2.png" alt="output_16_2.png" title="">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示第三次卷积后的输出数据以及相应的权值（filter）,取前１024个进行显示</span></span><br><span class="line">show_data(net.blobs[<span class="string">'conv3'</span>].data[<span class="number">0</span>],padval=<span class="number">0.5</span>)</span><br><span class="line"><span class="keyword">print</span> net.blobs[<span class="string">'conv3'</span>].data.shape</span><br><span class="line">show_data(net.params[<span class="string">'conv3'</span>][<span class="number">0</span>].data.reshape(<span class="number">64</span>*<span class="number">32</span>,<span class="number">5</span>,<span class="number">5</span>)[:<span class="number">1024</span>])</span><br><span class="line"><span class="keyword">print</span> net.params[<span class="string">'conv3'</span>][<span class="number">0</span>].data.shape</span><br></pre></td></tr></table></figure>
<pre><code>(1L, 64L, 8L, 8L)
(64L, 32L, 5L, 5L)
</code></pre><img src="/2016/08/08/caffe-cifar10-model-visualization-with-python/output_17_1.png" alt="output_17_1.png" title="">
<img src="/2016/08/08/caffe-cifar10-model-visualization-with-python/output_17_2.png" alt="output_17_2.png" title="">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示第三次池化后的输出数据</span></span><br><span class="line">show_data(net.blobs[<span class="string">'pool3'</span>].data[<span class="number">0</span>],padval=<span class="number">0.2</span>)</span><br><span class="line"><span class="keyword">print</span> net.blobs[<span class="string">'pool3'</span>].data.shape</span><br></pre></td></tr></table></figure>
<pre><code>(1L, 64L, 4L, 4L)
</code></pre><img src="/2016/08/08/caffe-cifar10-model-visualization-with-python/output_18_1.png" alt="output_18_1.png" title="">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最后一层输入属于某个类的概率</span></span><br><span class="line">feat = net.blobs[<span class="string">'prob'</span>].data[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">print</span> feat</span><br><span class="line">plt.plot(feat.flat)</span><br></pre></td></tr></table></figure>
<pre><code>[ 0.00170287  0.00115923  0.0225699   0.60395384  0.00453733  0.14171894
  0.00307363  0.01260873  0.15008588  0.05858969]

[&lt;matplotlib.lines.Line2D at 0x3bd38080&gt;]
</code></pre>
<p>与cifar10中的10种类型名称进行对比：</p>
<p>airplane、automobile、bird、cat、deer、dog、frog、horse、ship、truck</p>
<p>根据测试结果，判断为Cat。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[JS函数作用域链]]></title>
      <url>http://wentaoma.com/2016/08/05/JS-function-scope-chain/</url>
      <content type="html"><![CDATA[<p>本文分析JS的函数作用域链，并从此角度理解闭包</p>
<p>关于JS作用域，请阅读<a href="http://yanhaijing.com/javascript/2014/04/30/JavaScript-Scoping-and-Hoisting/" target="_blank" rel="external">JavaScript的作用域和提升机制</a><br><a id="more"></a></p>
<h2 id="u51FD_u6570_u4F5C_u7528_u57DF_u94FE"><a href="#u51FD_u6570_u4F5C_u7528_u57DF_u94FE" class="headerlink" title="函数作用域链"></a>函数作用域链</h2><p>当某个函数第一次被调用时，会创建一个执行环境(execution context)以及相应作用域链(scope chain)。<br>然后使用this, arguments和函数中其它命名参数的值来初始化函数的活动对象(activation object)做变量对象使用并放在<br>作用域链的首位。而作用域链的终点则是全局执行环境的变量对象。</p>
<p>对于每一个执行环境而言，都有一个表示(自己作用域中)变量(引用)的对象，称为变量对象。全局环境中变量对象<br>始终存在，而其它函数的变量对象只在执行过程中以活动对象形式存在。</p>
<p>在函数执行过程中，就按作用域链的顺序查找变量。</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">compare</span>(<span class="params">value1, value2</span>)</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(value1 &lt; value2)&#123;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;<span class="keyword">else</span> <span class="keyword">if</span>(value1 &gt; value2)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> result = compare(<span class="number">5</span>, <span class="number">10</span>);</span><br></pre></td></tr></table></figure>
<p>上述代码在调用<code>compare(5, 10)</code>时，<code>compare</code>函数执行环境与作用域链如下图所示：</p>
<img src="/2016/08/05/JS-function-scope-chain/js-function-scope-chain.png" alt="js-function-scope-chain.png" title="">
<p>作用域链本质其实是一个指向变量对象的指针列表。</p>
<p>实际上，在创建(声明)函数时，会预先创建一个包含全局变量对象的作用域链，保存在内部[[scope]]属性中。<br>在调用函数时，将[[scope]]中内容复制到函数执行环境作用域链中，并在作用域链前端添加当前函数活动对象。</p>
<p>一般来说当函数执行完毕后其活动对象就会被销毁，内存中仅保存全局变量对象。但是闭包情况则是例外。</p>
<h2 id="u51FD_u6570_u4F5C_u7528_u57DF_u94FE_u4E0E_u95ED_u5305"><a href="#u51FD_u6570_u4F5C_u7528_u57DF_u94FE_u4E0E_u95ED_u5305" class="headerlink" title="函数作用域链与闭包"></a>函数作用域链与闭包</h2><p>在一个函数内部定义的函数，在其创建(即外部函数执行)时，会将外部函数的活动对象添加到自己的作用域链[[scope]]中。<br>从而可以访问外部函数中定义的所有变量。<br>如果外部函数，这个内部函数被返回，因为内部函数作用域链仍然引用着外部函数活动对象，所以虽然外部函数在执行完毕后，<br>执行环境的作用域链会被销毁，但其活动对象仍然留在内存中。</p>
<p>这种携带包含它的函数的作用域的函数也就是闭包。</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">createComparisonFunction</span>(<span class="params">propertyName</span>)</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="function"><span class="keyword">function</span>(<span class="params">object1, object2</span>)</span>&#123;</span><br><span class="line">        <span class="keyword">var</span> value1 = object1[propertyName];</span><br><span class="line">        <span class="keyword">var</span> value2 = object2[propertyName];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(value1 &lt; value2)&#123;</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(value1 &gt; value2)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建函数(闭包)</span></span><br><span class="line"><span class="keyword">var</span> compareNames = createComparisonFunction(<span class="string">"name"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//调用函数</span></span><br><span class="line"><span class="keyword">var</span> result = compareNames(&#123;name: <span class="string">"Amy"</span>&#125;, &#123;name: <span class="string">"Bob"</span>&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//解除引用，以便释放内存</span></span><br><span class="line">compareNames = <span class="literal">null</span>;</span><br></pre></td></tr></table></figure>
<p>上面代码中<code>compareNames</code>引用的匿名函数执行环境作用域链如下图所示：</p>
<img src="/2016/08/05/JS-function-scope-chain/js-closure-scope-chain.png" alt="js-closure-scope-chain.png" title="">
<h3 id="u6CE8_u610F_u4E8B_u9879"><a href="#u6CE8_u610F_u4E8B_u9879" class="headerlink" title="注意事项"></a>注意事项</h3><h4 id="1-__u95ED_u5305_u4E0E_u53D8_u91CF"><a href="#1-__u95ED_u5305_u4E0E_u53D8_u91CF" class="headerlink" title="1. 闭包与变量"></a>1. 闭包与变量</h4><p>闭包引用的外部函数的整个变量对象(活动对象)，(变量对象)其中保存着对函数内部所有变量的引用而不是变量的值，因而<br>其只能取得外部函数中变量在执行完后的最后一个值。</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">createFunctions</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">    <span class="keyword">var</span> result = [];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">var</span> i=<span class="number">0</span>; i&lt;<span class="number">10</span>, i++)&#123;</span><br><span class="line">        result[i] = <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">            <span class="keyword">return</span> i;</span><br><span class="line">        &#125;;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>因为这一点，上述函数返回的函数数组中每一个调用返回的i值都为10.</p>
<h4 id="2-_this_u53D8_u91CF"><a href="#2-_this_u53D8_u91CF" class="headerlink" title="2. this变量"></a>2. this变量</h4><p>每个函数被调用时，其活动对象都会自动取得两个特殊变量：<code>this</code>,<code>arguments</code>。因此内部函数在作用域链上<br>搜索这两个变量时永远也不可能直接访问到外部函数中的这两个变量。不过可以通过将this保存在一个内部函数能够访问的<br>变量里，让其访问到该对象。</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> name = <span class="string">"Window"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> object = &#123;</span><br><span class="line">    name: <span class="string">"Object"</span>,</span><br><span class="line">    getNameFunction: <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">this</span>.name;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">alert(object.getNameFunction()()); <span class="comment">//"Window"</span></span><br></pre></td></tr></table></figure>
<h4 id="3-__u5185_u5B58_u6CC4_u6F0F"><a href="#3-__u5185_u5B58_u6CC4_u6F0F" class="headerlink" title="3. 内存泄漏"></a>3. 内存泄漏</h4><p>IE9以前版本对JS对象和DOM对象使用不同垃圾回收例程。在这些IE版本中如果闭包作用域链保存这HTML元素，<br>则该元素无法被正确回收。</p>
<h2 id="u76F8_u5173_u6587_u7AE0"><a href="#u76F8_u5173_u6587_u7AE0" class="headerlink" title="相关文章"></a>相关文章</h2><p><a href="http://wwsun.github.io/posts/scope-and-context-in-javascript.html" target="_blank" rel="external">理解JavaScript中的作用域和上下文</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[JS继承方式总结]]></title>
      <url>http://wentaoma.com/2016/08/04/JS-inherit-conclusion/</url>
      <content type="html"><![CDATA[<p>根据《JS高级程序设计·第三版》第六章内容，总结了下JS的继承方式<br><a id="more"></a></p>
<h1 id="Part1-__u7406_u89E3Prototype_u539F_u578B_u5BF9_u8C61"><a href="#Part1-__u7406_u89E3Prototype_u539F_u578B_u5BF9_u8C61" class="headerlink" title="Part1. 理解Prototype原型对象"></a>Part1. 理解Prototype原型对象</h1><p>无论什么时候只要创建了一个新函数，JS就会为该函数创建一个prototype属性指向此函数原型对象。<br>同时，原型对象会自动获得一个constructor属性，指向prototype属性所在函数指针。<br>当创建一个构造函数，并调用该构造函数创建一个新实例后，实例内部也将有一个指针（ES5中叫做[[Prototype]]），<br>Firefox，Safari，Chrome上为__proto__属性，其指向构造函数的原型对象。<br>其在原型对象上定义的属性为所有对象实例共享，称为原型属性。</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">Person</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Person.prototype.name = <span class="string">"Nicholas"</span>;</span><br><span class="line">Person.prototype.sayName = <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">    alert(<span class="keyword">this</span>.name);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> person1 = <span class="keyword">new</span> Person();</span><br><span class="line"><span class="keyword">var</span> person2 = <span class="keyword">new</span> Person();</span><br></pre></td></tr></table></figure>
<p>上述代码的实例、构造函数、和原型对象之间关系如下图所示：</p>
<img src="/2016/08/04/JS-inherit-conclusion/js_understand_prototype.png" alt="js_understand_prototype.png" title="">
<h1 id="Part2-_JS_u7EE7_u627F_u65B9_u5F0F_u603B_u7ED3"><a href="#Part2-_JS_u7EE7_u627F_u65B9_u5F0F_u603B_u7ED3" class="headerlink" title="Part2. JS继承方式总结"></a>Part2. JS继承方式总结</h1><h2 id="u539F_u578B_u94FE_u7EE7_u627F"><a href="#u539F_u578B_u94FE_u7EE7_u627F" class="headerlink" title="原型链继承"></a>原型链继承</h2><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">SuperType</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.property = <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">SuperType.prototype.getSuperValue = <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>.property;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">SubType</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.subproperty = <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//直接重写prototype对象实现继承SuperType</span></span><br><span class="line">SubType.prototype = <span class="keyword">new</span> SuperType();</span><br><span class="line"></span><br><span class="line">Sub.prototype.getSUbValue = <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>.subproperty;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> instance = <span class="keyword">new</span> SubType();</span><br><span class="line">alert(instance.getSuperValue()); <span class="comment">//true</span></span><br></pre></td></tr></table></figure>
<p>上述代码的继承是通过以一个<code>SuperType</code>实例替换<code>SubType</code>默认原型实现。<br>要注意此时：<code>instance.constructor</code>指向<code>SuperType</code>.</p>
<h3 id="u5B58_u5728_u7684_u95EE_u9898"><a href="#u5B58_u5728_u7684_u95EE_u9898" class="headerlink" title="存在的问题"></a>存在的问题</h3><ol>
<li><p>父类中实例属性成为子类原型属性，被子类所有实例共享。</p>
</li>
<li><p>创建子类型实例时，不能向父类型构造函数传递参数。</p>
</li>
</ol>
<h2 id="u501F_u7528_u6784_u9020_u51FD_u6570_u7EE7_u627F_28_u7ECF_u5178/_u4F2A_u9020_u5BF9_u8C61_u7EE7_u627F_29"><a href="#u501F_u7528_u6784_u9020_u51FD_u6570_u7EE7_u627F_28_u7ECF_u5178/_u4F2A_u9020_u5BF9_u8C61_u7EE7_u627F_29" class="headerlink" title="借用构造函数继承(经典/伪造对象继承)"></a>借用构造函数继承(经典/伪造对象继承)</h2><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">SuperType</span>(<span class="params">name</span>)</span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.name = name;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">SubType</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">    <span class="comment">//调用父类构造函数实现继承，并可以传递参数</span></span><br><span class="line">    SuperType.call(<span class="keyword">this</span>, <span class="string">"Nicholas"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//再添加子类实例属性、方法</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> instance = <span class="keyword">new</span> SubType();</span><br><span class="line">alert(instance.name); <span class="comment">//"Nicholas"</span></span><br></pre></td></tr></table></figure>
<p>借用构造函数继承是通过在子类型构造函数内部使用apply或call调用父类型构造函数，<br>从而在新创建对象上执行父类构造函数来实现。其可以向父类型构造函数传递参数。</p>
<h3 id="u5B58_u5728_u7684_u95EE_u9898-1"><a href="#u5B58_u5728_u7684_u95EE_u9898-1" class="headerlink" title="存在的问题"></a>存在的问题</h3><p>父类型原型中定义的方法对子类型不可见。</p>
<h2 id="u7EC4_u5408_u7EE7_u627F"><a href="#u7EC4_u5408_u7EE7_u627F" class="headerlink" title="组合继承"></a>组合继承</h2><p>也称为伪经典继承，将原型链和借用构造函数继承组合起来使用，是最常用的继承方法。</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">SuperType</span>(<span class="params">name</span>)</span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.name = name;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">SuperType.prototype.sayName = <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">    alert(<span class="keyword">this</span>.name);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">SubType</span>(<span class="params">name, age</span>)</span>&#123;</span><br><span class="line">    <span class="comment">//借用构造函数继承实例属性</span></span><br><span class="line">    SuperType.call(<span class="keyword">this</span>, name);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.age = age;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//原型链继承原型方法</span></span><br><span class="line">SubType.prototype = <span class="keyword">new</span> SuperType();</span><br></pre></td></tr></table></figure>
<h3 id="u539F_u578B_u5F0F_u7EE7_u627F"><a href="#u539F_u578B_u5F0F_u7EE7_u627F" class="headerlink" title="原型式继承"></a>原型式继承</h3><p>不需要额外创建新的构造函数，让新对象与原对象保持类似，但新对象会共享原对象的所有属性。</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">object</span>(<span class="params">o</span>)</span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">F</span>(<span class="params"></span>)</span>&#123;&#125; <span class="comment">//创建一个临时性构造函数</span></span><br><span class="line">    F.prototype = o; <span class="comment">//设置原型</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> F(); <span class="comment">//返回临时类型实例</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> person = &#123;</span><br><span class="line">    name: <span class="string">"Nicholas"</span>,</span><br><span class="line">    friends: [<span class="string">"Amy"</span>, <span class="string">"Bruno"</span>, <span class="string">"Candy"</span>]</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> subPerson1 = object(person);</span><br><span class="line">subPerson1.friends.push(<span class="string">"Danel"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> subPerson2 = object(person);</span><br><span class="line">subPerson2.friends.push(<span class="string">"Ely"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//因为subPerson1, subPerson2共享同一个原型对象，所以：</span></span><br><span class="line">alert(subPerson2.friends); <span class="comment">//Amy, Bruno, Candy, Danel, Ely</span></span><br><span class="line">alert(Person.friends); <span class="comment">//Amy, Bruno, Candy, Danel, Ely</span></span><br></pre></td></tr></table></figure>
<p>ES5中<code>Object.create()</code>方法规范化了原型式继承。<br>其接收两个参数:</p>
<ol>
<li><p>用做新对象原型的对象</p>
</li>
<li><p>[可选]为新对象定义额外属性的对象</p>
</li>
</ol>
<h3 id="u5BC4_u751F_u5F0F_u7EE7_u627F"><a href="#u5BC4_u751F_u5F0F_u7EE7_u627F" class="headerlink" title="寄生式继承"></a>寄生式继承</h3><p>即创建一个仅用于封装继承过程的函数，该函数在函数内部以某种方式增强对象，最后返回此对象。<br>因为其仅在原对象基础上增强而不是自定义类型和构造函数，故称为寄生式继承。</p>
<p>但其不能做到函数复用。</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">object</span>(<span class="params">o</span>)</span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">function</span> <span class="title">F</span>(<span class="params"></span>)</span>&#123;&#125;</span><br><span class="line">    F.prototype = o;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> F();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">createAnother</span>(<span class="params">original</span>)</span>&#123;</span><br><span class="line">    <span class="keyword">var</span> clone = object(original); <span class="comment">//可以使用任何返回新对象的函数</span></span><br><span class="line">    clone.sayHi = <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;     <span class="comment">//增强返回对象</span></span><br><span class="line">        alert(<span class="string">"Hi"</span>);</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">return</span> clone; <span class="comment">//返回增强对象</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> person = &#123;</span><br><span class="line">    name: <span class="string">"Nicholas"</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> anotherPerson = createAnother(person);</span><br></pre></td></tr></table></figure>
<h3 id="u5BC4_u751F_u7EC4_u5408_u5F0F_u7EE7_u627F"><a href="#u5BC4_u751F_u7EC4_u5408_u5F0F_u7EE7_u627F" class="headerlink" title="寄生组合式继承"></a>寄生组合式继承</h3><p>前面说过，组合继承是JS最常用的继承模式，但其也存在一个问题：</p>
<p><em><em>无论什么情况下都会调用两次父类构造函数</em></em></p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">SuperType</span>(<span class="params">name</span>)</span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.name = name;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">SuperType.prototype.sayName = <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">    alert(<span class="keyword">this</span>.name);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">SubType</span>(<span class="params">name, age</span>)</span>&#123;</span><br><span class="line">    SuperType.call(<span class="keyword">this</span>, name);<span class="comment">//第二次调用</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.age = age;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">SubType.prototype = <span class="keyword">new</span> SuperType();<span class="comment">//第一次调用</span></span><br></pre></td></tr></table></figure>
<p>这样导致我们在子类的prototype上创建了无用的SuperType的实例属性，<br>因为所有的父类实例属性都会在第二次使用<code>call</code>调用时被覆盖</p>
<p>对于这个问题的解决办法就是：寄生组合式继承</p>
<p>其和组合式继承区别在于，就是采用寄生方式去用父类型的prototype去覆盖子类型的prototype。<br>而不是使用父类型的实例去重设子类型的prototype。</p>
<p>因为我们仅仅需要在父类型prototype上的原型属性，而父类型的实例属性则之后通过借用构造函数方式继承。</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">inheritPrototype</span>(<span class="params">subType, superType</span>)</span>&#123;</span><br><span class="line">    <span class="keyword">var</span> prototype = object(superType.prototype); <span class="comment">//创建父类型"副本"</span></span><br><span class="line">    prototype.constructor = subType; <span class="comment">//重设constructor属性</span></span><br><span class="line">    subType.prototype = prototype; <span class="comment">//设置子类型prototype</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">SuperType</span>(<span class="params">name</span>)</span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.name = name;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">SuperType.prototype.sayName = <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">    alert(<span class="keyword">this</span>.name);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">SubType</span>(<span class="params">name, age</span>)</span>&#123;</span><br><span class="line">    SuperType.call(<span class="keyword">this</span>, name); <span class="comment">//继承父类实例属性</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.age = age;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">inheritPrototype(SubType, SuperType); <span class="comment">//继承父类原型属性</span></span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Hello World, Hexo & NexT]]></title>
      <url>http://wentaoma.com/2016/07/31/hello-world/</url>
      <content type="html"><![CDATA[<p>Hexo &amp; NexT Theme Quick Start.<br><a id="more"></a></p>
<h2 id="Hexo_Quick_Start"><a href="#Hexo_Quick_Start" class="headerlink" title="Hexo Quick Start"></a>Hexo Quick Start</h2><p>Check <a href="https://hexo.io/docs/" target="_blank" rel="external">Hexo documentation</a> for more info.</p>
<h3 id="Create_a_new_post"><a href="#Create_a_new_post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run_server"><a href="#Run_server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate_static_files"><a href="#Generate_static_files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy_to_remote_sites"><a href="#Deploy_to_remote_sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
<h2 id="NexT_Quick_Start"><a href="#NexT_Quick_Start" class="headerlink" title="NexT Quick Start"></a>NexT Quick Start</h2><p>请查看<a href="http://theme-next.iissnan.com/getting-started.html" target="_blank" rel="external">中文文档</a>。</p>
<h2 id="Useful_Links"><a href="#Useful_Links" class="headerlink" title="Useful Links"></a>Useful Links</h2><ul>
<li><p><a href="http://www.arao.me/2015/hexo-next-theme-optimize-base/" target="_blank" rel="external">动动手指，NexT主题与Hexo更搭哦（基础篇）</a></p>
</li>
<li><p><a href="http://theme-next.iissnan.com/faqs.html" target="_blank" rel="external">NexT中文FAQ</a></p>
</li>
<li><p><a href="http://www.selfrebuild.net/2015/06/24/Github-Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/" target="_blank" rel="external">Github+Hexo搭建博客教程</a></p>
</li>
</ul>
]]></content>
    </entry>
    
  
  
</search>
